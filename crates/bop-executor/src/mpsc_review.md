# MPSC Queue Review (Foundations)

Audit date: current refactor plan section 1. Focus areas were cache-line layout, TLS producer caching, close semantics, and coverage gaps.

## Findings & Proposed Fixes
- **Cache-line/layout:** The segmented SPSC backing queue already pads producer/consumer state via `CachePadded` (`seg_spsc_dynamic.rs`). However, `Inner` in `mpsc.rs` keeps `AtomicPtr<SegSpsc<_>>` slots, `producer_count`, and `closed` flags adjacent without padding. Given that producers and the consumer mutate `producer_count`/`closed` concurrently, we should wrap those hot counters in `CachePadded` or split them into separate structs to avoid false sharing once the worker-service loop starts polling more aggressively.
  - *Proposed fix:* Introduce `CachePadded<AtomicUsize>` (and for `closed`) inside `Inner`, or co-locate them in a `struct Counters` aligned via `CachePadded`.
- **TLS producer caching:** `with_local_producer` registers a queue by incrementing `producer_count` twice (CAS at `mpsc.rs:305-316` and `fetch_add` at `mpsc.rs:345`), so the observable count drifts and eventually hits `MAX_PRODUCERS` prematurely. The registration path also logs via `eprintln!`, which will spam stdout in release builds, and the cached `CloseFn` is a no-op even though `SegSpsc::close()` exists. When the TLS cache drops, the queue stays open and the consumer never sees the close bit.
  - *Proposed fix:* Remove the second `fetch_add`, switch the TLS `CloseFn` to call `SegSpsc::close`, and delete the debug `eprintln!`. Add an early `self.is_closed()` guard so stalled producers get `PushError::Closed` immediately.
- **Close semantics:** `Sender::close`/`Receiver::close` only set the shared `closed` flag and immediately return `false` if any queue pointers remain (`mpsc.rs:566-609` and `mpsc.rs:679-722`), contradicting the doc-comment promise that they wait for drain. They also never notify the `SignalWaker`, so a blocked consumer thread may park forever when the last producer calls `close_immediate`. Finally, `create_sender` closes queues on drop (via `LocalSender::drop`), but the TLS path leaves the slot registered until the consumer observes emptiness, so shutdown relies on polling rather than explicit closes.
  - *Proposed fix:* Have `close` spin (with backoff) until `producer_count` hits zero or expose an async-friendly drain; ensure we wake the consumer (`self.inner.waker.notify_all()` or equivalent) when we flip the flag; and close TLS queues proactively via the cached `CloseFn`.
- **Testing & bench coverage:** The module only contains an empty `#[cfg(test)]` stub. We lack regression tests for the TLS cache, producer count accounting, and close semantics. There are also no microbenchmarks covering push/pop latency under contention.
  - *Proposed fix:* Add unit tests that (1) exercise producer registration/unregistration across threads, (2) verify `close` wakes the consumer and drains queues, and (3) ensure TLS drop closes queues. Add criterion benchmarks for single-threaded push/pop and multi-producer stress to verify cache padding changes.
