//! Multi-Raft Cluster Example
//!
//! This example demonstrates how to create a multi-raft cluster using the maniac-raft crate.
//! It shows:
//! - Creating multiple Raft groups that share networking and storage
//! - Setting up the TCP transport layer with connection multiplexing
//! - Creating and initializing a cluster with multiple nodes
//! - Proposing commands to different Raft groups
//! - Leader election and cluster membership changes
//!
//! ## Architecture
//!
//! ```text
//! ┌─────────────────────────────────────────────────────────────┐
//! │                         Node 1 (127.0.0.1:9001)             │
//! │  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐    │
//! │  │   Group 1     │  │   Group 2     │  │   Group 3     │    │
//! │  └───────┬───────┘  └───────┬───────┘  └───────┬───────┘    │
//! │          │                  │                  │            │
//! │  ┌───────┴──────────────────┴──────────────────┴───────┐    │
//! │  │           Multiplexed TCP Transport + Storage       │    │
//! │  └───────────────────────────┬─────────────────────────┘    │
//! └──────────────────────────────┼──────────────────────────────┘
//!                                │
//!        ┌───────────────────────┼───────────────────────┐
//!        │                       │                       │
//! ┌──────┴──────┐         ┌──────┴──────┐         ┌──────┴──────┐
//! │   Node 2    │         │   Node 3    │         │    ...      │
//! │ :9002       │         │ :9003       │         │             │
//! │ Group 1,2,3 │         │ Group 1,2,3 │         │             │
//! └─────────────┘         └─────────────┘         └─────────────┘
//! ```
//!
//! ## Running the Example
//!
//! This example runs a 3-node cluster with 3 Raft groups. Each node participates
//! in all groups, demonstrating how multi-raft enables horizontal scaling.

use futures::StreamExt;
use maniac_raft::ManiacRaftTypeConfig;
use maniac_raft::multi::codec::{FromCodec, RawBytes, ToCodec};
use maniac_raft::multi::{
    DefaultNodeRegistry, ManiacRpcServer, ManiacRpcServerConfig, ManiacTcpTransport,
    ManiacTcpTransportConfig, MultiRaftConfig, MultiRaftManager, MultiplexedLogStorage,
    MultiplexedStorageConfig, NodeAddressResolver,
};
use openraft::impls::BasicNode;
use openraft::storage::RaftStateMachine;
use openraft::type_config::async_runtime::watch::WatchReceiver;
use openraft::{Config, LogId, OptionalSend, SnapshotMeta, StoredMembership};
use serde::{Deserialize, Serialize};
use std::collections::BTreeMap;
use std::fmt;
use std::io::Cursor;
use std::net::SocketAddr;
use std::sync::Arc;
use std::time::Duration;

// =============================================================================
// Application Data Types
// =============================================================================

/// The application request type - a simple key-value store command.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum KvCommand {
    /// Set a key to a value
    Set { key: String, value: String },
    /// Delete a key
    Delete { key: String },
}

// Required by openraft::AppData
impl fmt::Display for KvCommand {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            KvCommand::Set { key, value } => write!(f, "SET {}={}", key, value),
            KvCommand::Delete { key } => write!(f, "DELETE {}", key),
        }
    }
}

/// The application response type.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum KvResponse {
    /// Operation succeeded
    Ok,
    /// Get result
    Value(Option<String>),
}

// Required by openraft::AppDataResponse
impl fmt::Display for KvResponse {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            KvResponse::Ok => write!(f, "OK"),
            KvResponse::Value(Some(v)) => write!(f, "{}", v),
            KvResponse::Value(None) => write!(f, "(nil)"),
        }
    }
}

// Implement codec traits for KvCommand to work with multi-raft
impl ToCodec<RawBytes> for KvCommand {
    fn to_codec(&self) -> RawBytes {
        // Use bincode for serialization
        let bytes = bincode::serde::encode_to_vec(self, bincode::config::standard())
            .expect("serialization should not fail");
        RawBytes(bytes)
    }
}

impl FromCodec<RawBytes> for KvCommand {
    fn from_codec(raw: RawBytes) -> Self {
        let (cmd, _): (KvCommand, _) =
            bincode::serde::decode_from_slice(&raw.0, bincode::config::standard())
                .expect("deserialization should not fail");
        cmd
    }
}

// =============================================================================
// Type Configuration
// =============================================================================

/// Our Raft type configuration using KvCommand and KvResponse
pub type KvTypeConfig = ManiacRaftTypeConfig<KvCommand, KvResponse>;

// =============================================================================
// State Machine
// =============================================================================

/// Simple in-memory key-value store state machine.
pub struct KvStateMachine {
    /// The group ID this state machine belongs to
    group_id: u64,
    /// The key-value store
    data: BTreeMap<String, String>,
    /// Last applied log ID
    last_applied: Option<LogId<KvTypeConfig>>,
    /// Last membership
    last_membership: StoredMembership<KvTypeConfig>,
}

impl KvStateMachine {
    pub fn new(group_id: u64) -> Self {
        Self {
            group_id,
            data: BTreeMap::new(),
            last_applied: None,
            last_membership: StoredMembership::default(),
        }
    }
}

impl RaftStateMachine<KvTypeConfig> for KvStateMachine {
    type SnapshotBuilder = Self;

    async fn applied_state(
        &mut self,
    ) -> Result<(Option<LogId<KvTypeConfig>>, StoredMembership<KvTypeConfig>), std::io::Error> {
        Ok((self.last_applied.clone(), self.last_membership.clone()))
    }

    async fn apply<Strm>(&mut self, mut entries: Strm) -> Result<(), std::io::Error>
    where
        Strm: futures::Stream<
                Item = Result<openraft::storage::EntryResponder<KvTypeConfig>, std::io::Error>,
            > + Unpin
            + OptionalSend,
    {
        while let Some(entry_result) = entries.next().await {
            let (entry, responder) = entry_result?;
            self.last_applied = Some(entry.log_id.clone());

            let response = match entry.payload {
                openraft::EntryPayload::Blank => KvResponse::Ok,
                openraft::EntryPayload::Normal(cmd) => match cmd {
                    KvCommand::Set { key, value } => {
                        println!("  [Group {}] SET {} = {}", self.group_id, key, value);
                        self.data.insert(key, value);
                        KvResponse::Ok
                    }
                    KvCommand::Delete { key } => {
                        println!("  [Group {}] DELETE {}", self.group_id, key);
                        self.data.remove(&key);
                        KvResponse::Ok
                    }
                },
                openraft::EntryPayload::Membership(m) => {
                    self.last_membership = StoredMembership::new(Some(entry.log_id.clone()), m);
                    KvResponse::Ok
                }
            };

            // Send the response if a responder is present
            if let Some(r) = responder {
                r.send(response);
            }
        }

        Ok(())
    }

    async fn get_snapshot_builder(&mut self) -> Self::SnapshotBuilder {
        // For simplicity, we return a clone
        KvStateMachine {
            group_id: self.group_id,
            data: self.data.clone(),
            last_applied: self.last_applied.clone(),
            last_membership: self.last_membership.clone(),
        }
    }

    async fn begin_receiving_snapshot(&mut self) -> Result<Cursor<Vec<u8>>, std::io::Error> {
        Ok(Cursor::new(Vec::new()))
    }

    async fn install_snapshot(
        &mut self,
        meta: &SnapshotMeta<KvTypeConfig>,
        snapshot: Cursor<Vec<u8>>,
    ) -> Result<(), std::io::Error> {
        // Deserialize the snapshot data
        let data: BTreeMap<String, String> =
            bincode::serde::decode_from_slice(snapshot.get_ref(), bincode::config::standard())
                .map(|(d, _)| d)
                .unwrap_or_default();

        self.data = data;
        self.last_applied = meta.last_log_id.clone();
        self.last_membership = meta.last_membership.clone();

        println!(
            "  [Group {}] Installed snapshot at {:?}",
            self.group_id, meta.last_log_id
        );

        Ok(())
    }

    async fn get_current_snapshot(
        &mut self,
    ) -> Result<Option<openraft::Snapshot<KvTypeConfig>>, std::io::Error> {
        // For this example, we don't implement snapshotting
        Ok(None)
    }
}

impl openraft::storage::RaftSnapshotBuilder<KvTypeConfig> for KvStateMachine {
    async fn build_snapshot(&mut self) -> Result<openraft::Snapshot<KvTypeConfig>, std::io::Error> {
        let data = bincode::serde::encode_to_vec(&self.data, bincode::config::standard())
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))?;

        let last_applied = self.last_applied.clone().unwrap_or(LogId {
            leader_id: openraft::impls::leader_id_adv::LeaderId {
                term: 0,
                node_id: 0,
            },
            index: 0,
        });

        let meta = SnapshotMeta {
            last_log_id: Some(last_applied),
            last_membership: self.last_membership.clone(),
            snapshot_id: format!(
                "snapshot-{}",
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_secs()
            ),
        };

        Ok(openraft::Snapshot {
            meta,
            snapshot: Cursor::new(data),
        })
    }
}

// =============================================================================
// Main Example
// =============================================================================

fn main() {
    use futures_lite::future::block_on;
    use tracing_subscriber::EnvFilter;

    // Initialize logging
    // Initialize logging
    // Use RUST_LOG env var if set, otherwise default to INFO to avoid chatty debug logs
    // Setup logging
    let env_filter = EnvFilter::try_from_default_env().unwrap_or_else(|_| EnvFilter::new("warn"));

    tracing_subscriber::fmt()
        .with_env_filter(env_filter)
        .with_target(true)
        .init();

    println!("=== Multi-Raft Cluster Example ===\n");
    println!("This example demonstrates:");
    println!("  1. Creating a multi-raft node");
    println!("  2. Running multiple independent Raft groups");
    println!("  3. Shared networking and storage infrastructure");
    println!("  4. Proposing commands to different groups\n");

    // Create the maniac runtime with multiple workers
    // Need at least 4 workers to handle concurrent TCP servers and RPC processing
    let runtime = maniac::runtime::new_multi_threaded(16, 65536).expect("Failed to create runtime");

    // Spawn our async task on the runtime
    let handle = runtime.spawn(async_main()).expect("Failed to spawn task");

    // Block on the handle to wait for completion (using futures_lite like other examples)
    block_on(handle);

    println!("exiting...");
}

/// Node configuration for a multi-raft node
struct NodeConfig {
    node_id: u64,
    addr: SocketAddr,
}

/// Type alias for our node registry
type NodeRegistry = DefaultNodeRegistry<u64>;

/// Type alias for our transport
type Transport = ManiacTcpTransport<KvTypeConfig>;

/// Type alias for our storage
type Storage = MultiplexedLogStorage<KvTypeConfig>;

/// Type alias for our manager
type Manager = MultiRaftManager<KvTypeConfig, Transport, Storage>;

/// Type alias for our RPC server
type RpcServer = ManiacRpcServer<KvTypeConfig, Transport, Storage>;

/// Create a node's infrastructure (storage, transport, manager)
async fn create_node(
    node_id: u64,
    node_addr: SocketAddr,
    base_dir: &std::path::Path,
    node_registry: Arc<NodeRegistry>,
) -> Arc<Manager> {
    // Create per-node data directory
    let node_dir = base_dir.join(format!("node-{}", node_id));
    std::fs::create_dir_all(&node_dir).expect("Failed to create node dir");

    // Create storage configuration with 4 shards
    let storage_config = MultiplexedStorageConfig {
        base_dir: node_dir.clone(),
        num_shards: 4,
        segment_size: 64 * 1024 * 1024, // 64MB segment files
        fsync_interval: None,
        max_cache_entries_per_group: 10000,
        max_record_size: Some(1024 * 1024),
    };

    // Create the sharded log storage
    let storage = MultiplexedLogStorage::<KvTypeConfig>::new(storage_config)
        .await
        .expect("Failed to create storage");

    println!("  Node {}: Created storage at {:?}", node_id, node_dir);

    // Create TCP transport
    let transport_config = ManiacTcpTransportConfig {
        connect_timeout: Duration::from_secs(5),
        // Set slightly lower than election timeout (4000ms) to ensure transport
        // fails fast and rotates connections before Raft gives up.
        request_timeout: Duration::from_millis(3000),
        // Increased to allow more concurrent streams per pair
        connections_per_addr: 8,
        max_concurrent_requests_per_conn: 128,
        connection_ttl: Duration::from_secs(300),
        tcp_nodelay: true,
    };

    let transport = Transport::new(transport_config, node_registry);

    println!("  Node {}: Created TCP transport at {}", node_id, node_addr);

    // Create multi-raft configuration
    let multi_config = MultiRaftConfig {
        heartbeat_interval: Duration::from_millis(100),
    };

    // Create and return the manager wrapped in Arc
    Arc::new(MultiRaftManager::new(transport, storage, multi_config))
}

const NUM_GROUPS: u64 = 3;

async fn async_main() {
    // Create temporary data directory
    let temp_dir = std::env::temp_dir().join("multi-raft-example");
    let _ = std::fs::remove_dir_all(&temp_dir);
    std::fs::create_dir_all(&temp_dir).expect("Failed to create temp dir");

    println!("--- Setting up 3-Node Multi-Raft Cluster ---\n");

    // Define our 3 nodes
    let nodes = vec![
        NodeConfig {
            node_id: 1,
            addr: "127.0.0.1:9001".parse().unwrap(),
        },
        NodeConfig {
            node_id: 2,
            addr: "127.0.0.1:9002".parse().unwrap(),
        },
        NodeConfig {
            node_id: 3,
            addr: "127.0.0.1:9003".parse().unwrap(),
        },
    ];

    // Create shared node registry with all nodes registered
    let node_registry = Arc::new(NodeRegistry::new());
    for node in &nodes {
        node_registry.register(node.node_id, node.addr);
        println!("  Registered node {} at {}", node.node_id, node.addr);
    }
    println!();

    // Create Raft configuration (shared across all nodes)
    // Use longer timeouts to give TCP transport time to deliver messages
    let raft_config = Arc::new(
        Config {
            heartbeat_interval: 500,
            election_timeout_min: 1000,
            election_timeout_max: 1500,
            ..Default::default()
        }
        .validate()
        .expect("Invalid raft config"),
    );

    // Create all node managers concurrently using maniac::spawn for true parallelism
    let mut handles = Vec::new();
    for node in &nodes {
        let node_id = node.node_id;
        let addr = node.addr;
        let temp_dir = temp_dir.clone();
        let node_registry = node_registry.clone();
        let handle = maniac::spawn(async move {
            let manager = create_node(node_id, addr, &temp_dir, node_registry).await;
            (node_id, addr, manager)
        })
        .expect("Failed to spawn node creation task");
        handles.push(handle);
    }

    // Wait for all nodes to be created
    let mut node_results = Vec::new();
    for handle in handles {
        node_results.push(handle.await);
    }

    // Start RPC servers for all nodes
    let mut managers = Vec::new();
    for (node_id, addr, manager) in node_results {
        // Create and start RPC server for this node
        let rpc_config = ManiacRpcServerConfig {
            bind_addr: addr,
            ..Default::default()
        };
        let rpc_server = Arc::new(RpcServer::new(rpc_config, manager.clone()));

        // Spawn the RPC server in the background
        let server = rpc_server.clone();
        let _ = maniac::spawn(async move {
            if let Err(e) = server.serve().await {
                eprintln!("RPC server error: {:?}", e);
            }
        });

        println!("  Node {}: Started RPC server at {}", node_id, addr);
        managers.push((node_id, manager));
    }

    // Give the servers a moment to bind
    maniac::time::sleep(Duration::from_millis(100)).await;

    println!("\n--- Creating Raft groups on all nodes ---\n");

    // Build membership with all 3 nodes
    let mut members = BTreeMap::new();
    for node in &nodes {
        members.insert(node.node_id, BasicNode::default());
    }

    // Add three Raft groups to each node
    for group_id in 1..=NUM_GROUPS {
        println!("Group {}:", group_id);

        for (node_id, manager) in &managers {
            let state_machine = KvStateMachine::new(group_id);

            match manager
                .add_group(group_id, *node_id, raft_config.clone(), state_machine)
                .await
            {
                Ok(_raft) => {
                    println!("  Node {}: Added group {}", node_id, group_id);
                }
                Err(e) => {
                    println!(
                        "  Node {}: Failed to add group {}: {:?}",
                        node_id, group_id, e
                    );
                }
            }
        }
        println!();
    }

    // Initialize cluster from node 1 (it will become the initial leader)
    println!("--- Initializing cluster from node 1 ---\n");

    let (_node1_id, node1_manager) = &managers[0];

    // Initialize groups sequentially for stability
    for group_id in 1..=NUM_GROUPS {
        if let Some(raft) = node1_manager.get_group(group_id) {
            match maniac::time::timeout(Duration::from_secs(5), raft.initialize(members.clone()))
                .await
            {
                Ok(Ok(_)) => {
                    println!("  Group {} initialized successfully", group_id);
                }
                Ok(Err(e)) => {
                    println!(
                        "  Warning: Could not initialize group {}: {:?}",
                        group_id, e
                    );
                }
                Err(_) => {
                    println!(
                        "  TIMEOUT: Initialize for group {} timed out after 5s",
                        group_id
                    );
                }
            }
        }
    }

    // Wait for leader election (need more time for 3 groups to elect leaders)
    println!("\n--- Waiting for leader election ---\n");
    maniac::time::sleep(Duration::from_secs(5)).await;

    // Check leader status for each group
    for group_id in 1..=NUM_GROUPS {
        for (node_id, manager) in &managers {
            if let Some(raft) = manager.get_group(group_id) {
                let metrics = raft.metrics().borrow_watched().clone();
                if metrics.current_leader == Some(*node_id) {
                    println!("  Group {}: Node {} is the LEADER", group_id, node_id);
                } else {
                    println!(
                        "  Group {}: Node {} is a follower (leader: {:?})",
                        group_id, node_id, metrics.current_leader
                    );
                }
            }
        }
    }

    // Show final state
    println!("\n--- Cluster Status ---\n");
    for (node_id, manager) in &managers {
        println!(
            "  Node {}: {} groups active",
            node_id,
            manager.group_count()
        );
    }

    // Keep cluster running for testing/interaction
    // Press Ctrl+C to shut down gracefully
    println!("\nCluster is running. Press Ctrl+C to shutdown...\n");
    loop {
        maniac::time::sleep(Duration::from_secs(60)).await;
    }

    // Cleanup
    println!("\n--- Shutting down ---\n");
    for (node_id, manager) in &managers {
        for group_id in 1..=NUM_GROUPS {
            manager.remove_group(group_id).await;
        }
        println!("  Node {}: Removed all groups", node_id);
    }

    let _ = std::fs::remove_dir_all(&temp_dir);
    println!("  Cleaned up temporary directory");

    println!("\n=== Example completed successfully! ===\n");
}

// =============================================================================
// Additional Documentation
// =============================================================================

/// # Multi-Raft Design Notes
///
/// ## Why Multi-Raft?
///
/// In a large-scale distributed system, using a single Raft group becomes a bottleneck:
/// - All reads/writes must go through a single leader
/// - A single log grows unboundedly
/// - Scaling to many nodes increases election latency
///
/// Multi-Raft solves this by partitioning data across multiple Raft groups:
/// - Each group handles a subset of the data (e.g., by key range or hash)
/// - Groups elect leaders independently
/// - Network and storage can be shared to reduce overhead
///
/// ## Shared Infrastructure
///
/// This implementation shares:
/// - **TCP Transport**: Connection pools are shared across groups
/// - **Log Storage**: Single log file with group_id prefix for entries
/// - **Configuration**: Heartbeat intervals, timeouts, etc.
///
/// ## Scaling Patterns
///
/// 1. **Horizontal scaling**: Add more nodes to each group
/// 2. **Sharding**: Split data across groups by key range
/// 3. **Placement**: Locate group leaders near their data
///
/// ## Example Use Cases
///
/// - **Key-Value Store**: Each group handles a key range
/// - **Message Queue**: Each group handles a topic/partition
/// - **Database**: Each group handles a table or shard
#[allow(dead_code)]
fn _documentation() {}
