//! Segmented SPSC (single-producer / single-consumer) bounded queue with zero-copy APIs.
//!
//! # Overview
//!
//! `SegSpsc` is a high-performance, bounded SPSC queue that combines:
//! - **Segmented storage** for flexible capacity without large contiguous allocations
//! - **Lazy allocation** to minimize memory footprint
//! - **Segment pooling** for cache-hot reuse (reduces allocator pressure)
//! - **Zero-copy APIs** for efficient in-place consumption
//! - **Close bit encoding** for graceful shutdown (bit 63 of head position)
//!
//! # Architecture
//!
//! ```text
//! ┌────────────────────────────────────────────────────────────────┐
//! │                    Segment Directory                           │
//! │  [Seg 0] [Seg 1] [Seg 2] ... [Seg N]  (NUM_SEGS slots)         │
//! └────┬──────────┬──────────┬─────────────────────────────────────┘
//!      │          │          │
//!      ▼          ▼          ▼
//!   Segment    Segment     null     Each segment: SEG_SIZE items
//!   [items]    [items]              Lazily allocated by producer
//!
//! Global Positions (monotonic u64):
//!   head: [63=close bit][62..0=position]  Producer writes here
//!   tail: position (no close bit)          Consumer reads here
//!
//! Segment Pooling:
//!   [sealable_lo, sealable_hi) = sealed segments available for reuse
//!   Producer steals from pool before allocating fresh memory
//! ```
//!
//! # Two-Level Indexing
//!
//! Each position `pos` maps to `(segment_index, offset)`:
//! ```text
//! segment_index = (pos >> P) & DIR_MASK
//! offset        = pos & SEG_MASK
//! ```
//!
//! # Segment Lifecycle
//!
//! 1. **Allocation**: Producer allocates when writing to new segment boundary
//! 2. **Active**: Segment contains data being produced/consumed
//! 3. **Sealed**: Consumer marks fully-consumed segments for reuse
//! 4. **Pooled**: Sits in [sealable_lo, sealable_hi) range
//! 5. **Reused**: Producer steals from pool (cache-hot memory)
//!
//! # Safety Constraints
//!
//! - **Single producer**: Only one thread may call producer methods (`try_push`, etc.)
//! - **Single consumer**: Only one thread may call consumer methods (`try_pop`, etc.)
//! - **T**: Simplifies implementation (no Drop concerns during segment reuse)
//!
//! # Const Parameters
//!
//! - `P`: log2(segment size). Example: P=8 → 256 items/segment
//! - `NUM_SEGS_P2`: log2(directory size). Example: NUM_SEGS_P2=10 → 1024 segments
//!
//! **Capacity** = `(SEG_SIZE × NUM_SEGS) - 1` (one-empty-slot rule)
//!
//! # Close Bit Encoding
//!
//! The high bit (bit 63) of the head position encodes the close state:
//! - **Clear**: Queue is open
//! - **Set**: Queue is closed (via `close()`)
//!
//! This allows lock-free close detection without a separate atomic flag.
//!
//! # Performance Characteristics
//!
//! - **Push/Pop**: O(1) amortized (lazy allocation on segment boundaries)
//! - **Segment reuse**: O(1) steal from sealed pool (cache-friendly)
//! - **Zero-copy consume**: No memcpy, just slice access
//! - **Cache optimization**: Separate cache lines for producer/consumer state
//!
//! # Example
//!
//! ```ignore
//! use bop_mpmc::seg_spsc::SegSpsc;
//!
//! // 64 items/segment, 256 segments = 16,383 capacity
//! type Queue = SegSpsc<usize, 6, 8>;
//!
//! let queue = Queue::new();
//!
//! // Producer
//! queue.try_push(42).unwrap();
//! queue.try_push_n(&[1, 2, 3]).unwrap();
//!
//! // Consumer (zero-copy)
//! let consumed = queue.consume_in_place(10, |slice| {
//!     for &item in slice {
//!         println!("{}", item);
//!     }
//!     slice.len()  // Consumed all
//! });
//!
//! // Close and drain
//! queue.close();
//! while let Some(item) = queue.try_pop() {
//!     // Process remaining items
//! }
//! ```

use core::cell::UnsafeCell;
use core::mem::MaybeUninit;
use core::ptr;
use core::sync::atomic::{AtomicPtr, AtomicU64, Ordering};
use std::alloc::{alloc_zeroed, dealloc, Layout};
use std::sync::Arc;

use crate::utils::CachePadded;

use crate::runtime::signal::SignalGate;
use crate::{PopError, PushError};

/// Cache-line aligned producer state (hot path for enqueue operations).
///
/// All producer-side atomics and caches are grouped here to minimize false sharing
/// with consumer state. The `CachePadded` wrapper ensures this struct occupies
/// its own cache line.
///
/// # Head Position Encoding
///
/// ```text
/// Bit 63: Close flag (1 = closed, 0 = open)
/// Bits 62..0: Monotonic position (never wraps in practice)
/// ```
///
/// # Tail Cache Optimization
///
/// `tail_cache` uses `UnsafeCell` instead of `AtomicU64` because:
/// 1. Only the producer thread accesses this field
/// 2. No inter-thread synchronization required
/// 3. Avoids atomic overhead (faster loads/stores)
/// 4. Refreshed from consumer's `tail` only when needed (on full)
///
/// This is **safe** because the producer is single-threaded (enforced by API design).
struct ProducerState {
	/// **Head position**: Where the next item will be written (includes close bit in bit 63).
	///
	/// Monotonically increasing (except close bit). Consumer reads this with `Acquire`
	/// to see producer's writes.
	head: AtomicU64,

	/// **Cached tail**: Producer's local copy of the consumer's tail position.
	///
	/// Refreshed only when queue appears full (avoids expensive cross-cache-line reads).
	/// Uses `UnsafeCell` for zero-cost single-threaded access.
	tail_cache: UnsafeCell<u64>,

	/// **Sealable low watermark**: Lower bound of the sealed segment pool.
	///
	/// Producer increments this when stealing a segment from [sealable_lo, sealable_hi).
	sealable_lo: AtomicU64,

	/// **Fresh allocation counter**: Number of segments allocated from the system allocator.
	///
	/// Metric for cache misses (segment not found in pool). High values indicate
	/// poor segment reuse (memory churn).
	fresh_allocations: AtomicU64,

	/// **Pool reuse counter**: Number of segments reused from the sealed pool.
	///
	/// Metric for cache hits (segment reused from hot pool). High values indicate
	/// good memory locality.
	pool_reuses: AtomicU64,

	/// **Total allocated segments**: Current number of segments that are allocated (non-null).
	///
	/// Incremented when allocating fresh segments, decremented when deallocating.
	/// This tracks the live memory footprint of the queue.
	total_allocated_segments: AtomicU64,
}

// SAFETY: ProducerState is only accessed by the single producer thread,
// so the UnsafeCell access is safe. The atomic head provides
// the necessary synchronization between producer and consumer.
unsafe impl Send for ProducerState {}
unsafe impl Sync for ProducerState {}

impl ProducerState {
	fn new() -> Self {
		Self {
			head: AtomicU64::new(0),
			tail_cache: UnsafeCell::new(0),
			sealable_lo: AtomicU64::new(0),
			fresh_allocations: AtomicU64::new(0),
			pool_reuses: AtomicU64::new(0),
			total_allocated_segments: AtomicU64::new(0),
		}
	}
}

/// Cache-line aligned consumer state (hot path for dequeue operations).
///
/// Mirrors `ProducerState` design: all consumer-side fields grouped together
/// to avoid false sharing. The `CachePadded` wrapper ensures separate cache
/// line from producer state.
///
/// # Head Cache Optimization
///
/// `head_cache` uses `UnsafeCell` for the same reasons as producer's `tail_cache`:
/// single-threaded access (consumer only), no synchronization needed, zero overhead.
/// Refreshed from producer's `head` only when queue appears empty.
struct ConsumerState {
	/// **Tail position**: Where the next item will be read from (no close bit).
	///
	/// Monotonically increasing. Producer reads this with `Acquire` to calculate
	/// available space.
	tail: AtomicU64,

	/// **Cached head**: Consumer's local copy of the producer's head position.
	///
	/// Refreshed only when queue appears empty (avoids cross-cache-line reads).
	/// Stores the raw head value (including close bit in bit 63).
	head_cache: UnsafeCell<u64>,

	/// **Sealable high watermark**: Upper bound of the sealed segment pool.
	///
	/// Consumer increments this when fully consuming a segment, making it
	/// available for producer to steal from [sealable_lo, sealable_hi).
	sealable_hi: AtomicU64,
}

// SAFETY: ConsumerState is only accessed by the single consumer thread,
// so the UnsafeCell access is safe. The atomic tail provides
// the necessary synchronization between producer and consumer.
unsafe impl Send for ConsumerState {}
unsafe impl Sync for ConsumerState {}

impl ConsumerState {
	fn new() -> Self {
		Self {
			tail: AtomicU64::new(0),
			head_cache: UnsafeCell::new(0),
			sealable_hi: AtomicU64::new(0),
		}
	}
}

/// **Producer half of a segmented SPSC (single-producer, single-consumer) queue.**
///
/// This struct provides the enqueue API for the lock-free SPSC queue. It maintains
/// an `Arc` reference to the shared `SegSpsc` state and implements the producer-side
/// operations: `try_push`, `try_push_n`, and allocation statistics tracking.
///
/// # Design Philosophy
///
/// - **Split Ownership**: Producer and consumer are separate types to enforce
///   single-producer/single-consumer semantics at compile time
/// - **Arc-based**: Uses `Arc` for shared state, allowing drop semantics and
///   proper cleanup when either half is dropped
/// - **Cache-optimized**: Producer state (head, tail_cache) is cache-padded
///   separately from consumer state to prevent false sharing
///
/// # Memory Model
///
/// The producer owns the "head" position (monotonically increasing u64):
/// ```text
/// Producer writes → [head, tail] → Consumer reads
///                    ↑queue data↑
/// ```
///
/// - **Head**: Published via `Ordering::Release` after data writes
/// - **Tail Cache**: Local copy of consumer's tail, refreshed with `Ordering::Acquire`
///
/// # Type Parameters
///
/// - `T`: Item type, must be `Copy` (allows lock-free memcpy semantics)
/// - `P`: Segment size exponent (segment_size = 2^P items)
/// - `NUM_SEGS_P2`: Number of segments exponent (num_segs = 2^NUM_SEGS_P2)
///
/// # Capacity
///
/// Total capacity: `(2^P × 2^NUM_SEGS_P2) - 1` items
///
/// Examples:
/// - `Sender<u64, 10, 0>`: 1023 items (1024 - 1)
/// - `Sender<u64, 13, 2>`: 32767 items (8192 × 4 - 1)
///
/// The "-1" ensures head==tail distinguishes empty from full.
///
/// # Segment Allocation
///
/// Segments are lazily allocated by the producer when crossing boundaries:
/// 1. **First write to segment**: Allocate via `alloc_segment()` (fresh allocation)
/// 2. **Subsequent writes**: Reuse from sealed pool if available (pool reuse)
///
/// Track allocation patterns via:
/// - `fresh_allocations()`: Count of new heap allocations
/// - `pool_reuses()`: Count of reuses from sealed segment pool
/// - `pool_reuse_rate()`: Ratio of reuses to total allocations
///
/// # Close Semantics
///
/// The queue can be closed via `SegSpsc::close()`:
/// - Sets bit 63 in the head position atomically
/// - All subsequent `try_push*` calls return `PushError::Closed`
/// - Consumer can still drain remaining items
///
/// # Thread Safety
///
/// - **Single Producer**: Only ONE thread may hold the `Sender`
/// - **Concurrent Consumer**: Safe concurrent operation with the `Receiver`
/// - **Send + !Sync**: Can be sent to another thread, but not shared
///
/// # Performance Characteristics
///
/// | Operation       | Latency (cache hit) | Latency (segment boundary) |
/// |-----------------|---------------------|----------------------------|
/// | `try_push`      | ~5-15 ns            | ~20-50 ns                  |
/// | `try_push_n`    | ~10-20 ns           | ~30-50 ns per boundary     |
/// | `len`           | ~2-5 ns             | N/A                        |
///
/// # Example
///
/// ```ignore
/// use bop_mpmc::SegSpsc;
///
/// type Queue = SegSpsc<u64, 10, 0>;  // 1023-item queue
/// let (producer, consumer) = Queue::new();
///
/// // Producer thread
/// std::thread::spawn(move || {
///     for i in 0..100 {
///         match producer.try_push(i) {
///             Ok(()) => {},
///             Err(e) => eprintln!("Push failed: {:?}", e),
///         }
///     }
/// });
///
/// // Consumer thread
/// std::thread::spawn(move || {
///     while let Some(item) = consumer.try_pop() {
///         println!("Got: {}", item);
///     }
/// });
/// ```
///
/// # See Also
///
/// - [`Receiver`]: The consumer half
/// - [`Spsc`]: The underlying shared queue structure
/// - [`PushError`]: Error types for push operations
pub struct Sender<T, const P: usize, const NUM_SEGS_P2: usize> {
	queue: Arc<Spsc<T, P, NUM_SEGS_P2>>,
}

impl<T, const P: usize, const NUM_SEGS_P2: usize> Sender<T, P, NUM_SEGS_P2> {
	/// Returns the number of items currently in the queue.
	///
	/// This is calculated as `head - tail`, where both positions are monotonically
	/// increasing u64 counters. The close bit is masked out before subtraction.
	///
	/// # Close Bit Handling
	///
	/// The head position may have the close bit (bit 63) set. This method masks it out:
	/// ```ignore
	/// let h_pos = h & RIGHT_MASK;  // Clear bit 63
	/// let length = h_pos.saturating_sub(t);  // Prevent underflow
	/// ```
	///
	/// # Consistency
	///
	/// Uses `Ordering::Acquire` for both loads to ensure visibility of preceding writes.
	/// However, the value may be stale by the time the caller uses it (concurrent operations).
	///
	/// # Performance
	///
	/// ~2-5 ns (two atomic loads + subtraction)
	///
	/// # Example
	///
	/// ```ignore
	/// queue.try_push(1)?;
	/// queue.try_push(2)?;
	/// assert_eq!(queue.len(), 2);
	/// ```
	#[inline(always)]
	pub fn len(&self) -> usize {
		self.queue.len()
	}

	/// Returns `true` if the queue contains no items.
	///
	/// Equivalent to `self.len() == 0`, but more readable.
	///
	/// # Note
	///
	/// This check is not atomic with subsequent operations. A producer may enqueue
	/// items immediately after this returns `true`.
	///
	/// # Example
	///
	/// ```ignore
	/// assert!(queue.is_empty());
	/// queue.try_push(1)?;
	/// assert!(!queue.is_empty());
	/// ```
	#[inline(always)]
	pub fn is_empty(&self) -> bool {
		self.queue.is_empty()
	}

	/// Returns `true` if the queue is at maximum capacity.
	///
	/// When the queue is full, `try_push()` and `try_push_n()` will return `PushError::Full`.
	///
	/// # Capacity
	///
	/// The capacity is `(segment_size * num_segments) - 1`:
	/// - For `SegSpsc<T, 10, 0>`: `1024 - 1 = 1023` items
	/// - For `SegSpsc<T, 13, 2>`: `(8192 * 4) - 1 = 32767` items
	///
	/// # Note
	///
	/// Like `is_empty()`, this is not atomic with subsequent operations.
	///
	/// # Example
	///
	/// ```ignore
	/// for i in 0..queue.capacity() {
	///     queue.try_push(i).unwrap();
	/// }
	/// assert!(queue.is_full());
	/// assert!(queue.try_push(999).is_err());  // Returns PushError::Full
	/// ```
	#[inline(always)]
	pub fn is_full(&self) -> bool {
		self.queue.is_full()
	}

	/// Attempts to enqueue a single item.
	///
	/// This is a convenience wrapper around `try_push_n()` for single-item operations.
	///
	/// # Returns
	///
	/// - `Ok(())`: Item successfully enqueued
	/// - `Err(PushError::Full(item))`: Queue is at capacity, item returned
	/// - `Err(PushError::Closed(item))`: Queue has been closed, item returned
	///
	/// # Close Semantics
	///
	/// If the queue is closed (bit 63 of head is set), this returns `Err(PushError::Closed)`.
	/// The close check happens atomically before any capacity calculations.
	///
	/// # Signal Integration
	///
	/// After successful enqueue, automatically calls `schedule()` if a signal gate is configured.
	/// This wakes the executor to process the newly available item.
	///
	/// # Performance
	///
	/// ~5-15 ns for successful push (cache hit, no segment boundary crossing)
	/// ~20-50 ns for segment allocation (first push to a new segment)
	///
	/// # Example
	///
	/// ```ignore
	/// match queue.try_push(42) {
	///     Ok(()) => println!("Enqueued successfully"),
	///     Err(PushError::Full(item)) => println!("Queue full, item={}", item),
	///     Err(PushError::Closed(item)) => println!("Queue closed, item={}", item),
	/// }
	/// ```
	///
	/// # Thread Safety
	///
	/// Safe to call from the single producer thread. NOT safe for concurrent producers
	/// (SPSC guarantee must be maintained by the caller).
	#[inline(always)]
	pub fn try_push(&self, item: T) -> Result<(), PushError<T>> {
		self.queue.try_push(item)
	}

	/// Attempts to enqueue multiple items from a slice.
	///
	/// Copies items from `src` into the queue, potentially spanning multiple segments.
	/// Returns the number of items successfully copied (may be partial if queue fills).
	///
	/// # Algorithm
	///
	/// 1. **Close Check**: Return `Err(Closed)` if bit 63 is set in head
	/// 2. **Capacity Check**: Calculate free space using cached tail (refresh if needed)
	/// 3. **Bulk Copy**: Copy min(src.len(), free) items across segment boundaries
	/// 4. **Head Update**: Atomically publish new head position with Release ordering
	/// 5. **Signal**: Call `schedule()` to wake executor
	///
	/// # Tail Caching
	///
	/// The producer maintains a cached copy of the consumer's tail position:
	/// - **Fast path**: If cached tail shows available space, skip atomic load
	/// - **Slow path**: If cached tail shows full, refresh from consumer's atomic tail
	///
	/// This optimization eliminates cache line bouncing in the common case where the
	/// queue has available space.
	///
	/// # Segment Boundaries
	///
	/// Items may span multiple segments. For each segment:
	/// ```ignore
	/// seg_idx = (position >> P) & DIR_MASK;  // Which segment in directory
	/// offset = position & SEG_MASK;          // Offset within segment
	/// ```
	///
	/// When crossing a boundary (offset wraps to 0), the next segment is lazily allocated
	/// via `ensure_segment_for()`.
	///
	/// # Memory Ordering
	///
	/// - Tail cache: `Acquire` (when refreshing from consumer)
	/// - Head update: `Release` (publishes data + position to consumer)
	///
	/// # Returns
	///
	/// - `Ok(n)`: `n` items successfully enqueued (0 ≤ n ≤ src.len())
	/// - `Err(PushError::Full(()))`: Queue full, no items enqueued
	/// - `Err(PushError::Closed(()))`: Queue closed, no items enqueued
	///
	/// # Partial Success
	///
	/// If `src.len() > free`, only `free` items are copied and `Ok(free)` is returned.
	/// The caller must check the return value to handle partial writes:
	///
	/// ```ignore
	/// let items = vec![1, 2, 3, 4, 5];
	/// match queue.try_push_n(&items) {
	///     Ok(n) if n == items.len() => println!("All items enqueued"),
	///     Ok(n) => println!("Partial: enqueued {}/{}", n, items.len()),
	///     Err(PushError::Full(())) => println!("Queue full"),
	///     Err(PushError::Closed(())) => println!("Queue closed"),
	/// }
	/// ```
	///
	/// # Performance
	///
	/// - **Single segment**: ~10-20 ns (one memcpy, no allocation)
	/// - **Multiple segments**: ~30-50 ns per segment boundary (allocation + memcpy)
	/// - **Throughput**: ~500M-1B ops/sec on modern x86_64 (depending on item size)
	///
	/// # Safety
	///
	/// Uses `ptr::copy_nonoverlapping` internally. Safety invariants:
	/// - Source slice is valid for reads
	/// - Destination segment is allocated and valid for writes
	/// - No overlap between source and destination (guaranteed by design)
	/// - Size calculation accounts for `sizeof::<T>()`
	#[inline(always)]
	pub fn try_push_n(&self, src: &[T]) -> Result<usize, PushError<()>> {
		self.queue.try_push_n(src)
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Metrics API
	// ──────────────────────────────────────────────────────────────────────────────

	/// Returns the number of fresh segment allocations (pool misses).
	///
	/// This metric tracks how many times `ensure_segment_for()` had to allocate new
	/// memory from the system allocator because no sealed segment was available for reuse.
	///
	/// # Interpretation
	///
	/// - **High value**: Indicates working set exceeds pooled segment capacity (more allocations)
	/// - **Low value**: Indicates effective pooling (segments are being reused efficiently)
	///
	/// # Use Cases
	///
	/// - **Performance tuning**: Compare fresh vs reused to assess pool effectiveness
	/// - **Capacity planning**: High fresh count may indicate need for larger `NUM_SEGS_P2`
	/// - **Memory profiling**: Track allocation patterns over time
	///
	/// # Example
	///
	/// ```ignore
	/// queue.reset_allocation_stats();
	/// // ... run workload ...
	/// println!("Fresh allocations: {}", queue.fresh_allocations());
	/// println!("Pool reuse rate: {:.1}%", queue.pool_reuse_rate());
	/// ```
	#[inline(always)]
	pub fn fresh_allocations(&self) -> u64 {
		self.queue.fresh_allocations()
	}

	/// Returns the number of segment pool reuses (pool hits).
	///
	/// This metric tracks how many times `ensure_segment_for()` successfully reused a
	/// sealed segment from the pool instead of allocating new memory.
	///
	/// # Interpretation
	///
	/// - **High value**: Indicates effective pooling and good cache locality
	/// - **Low value**: May indicate insufficient sealed segments or high churn rate
	///
	/// # Pool Reuse Mechanism
	///
	/// Segments in the range [sealable_lo, sealable_hi) are available for reuse.
	/// When the producer needs a new segment, it first checks this range. If available,
	/// the segment is moved (not copied) to the target slot in the directory.
	///
	/// # Example
	///
	/// ```ignore
	/// let before = queue.pool_reuses();
	/// // ... enqueue/dequeue cycle that triggers segment reuse ...
	/// let after = queue.pool_reuses();
	/// println!("Segments reused: {}", after - before);
	/// ```
	#[inline(always)]
	pub fn pool_reuses(&self) -> u64 {
		self.queue.pool_reuses()
	}

	/// Returns the segment pool reuse rate as a percentage (0-100%).
	///
	/// Calculated as: `(pool_reuses / (fresh_allocations + pool_reuses)) * 100`
	///
	/// # Interpretation
	///
	/// - **0%**: No reuse (all allocations are fresh) - may indicate cold start or insufficient pool
	/// - **50%**: Half of allocations are reused - moderate pooling effectiveness
	/// - **90%+**: Excellent reuse - pool is working well, high cache locality
	///
	/// # Performance Impact
	///
	/// - **Fresh allocation**: ~100-500 ns (system allocator + initialization)
	/// - **Pool reuse**: ~5-10 ns (pointer swap)
	///
	/// High reuse rates directly translate to lower latency and higher throughput.
	///
	/// # Example
	///
	/// ```ignore
	/// // Warm up the pool
	/// for _ in 0..queue.capacity() {
	///     queue.try_push(0)?;
	/// }
	/// for _ in 0..queue.capacity() {
	///     queue.try_pop();
	/// }
	///
	/// queue.reset_allocation_stats();
	///
	/// // Run steady-state workload
	/// for _ in 0..1_000_000 {
	///     queue.try_push(42)?;
	///     queue.try_pop();
	/// }
	///
	/// println!("Reuse rate: {:.1}%", queue.pool_reuse_rate());
	/// // Expected: ~99% after warm-up phase
	/// ```
	#[inline(always)]
	pub fn pool_reuse_rate(&self) -> f64 {
		self.queue.pool_reuse_rate()
	}

	/// Resets allocation statistics to zero.
	///
	/// Useful for:
	/// - Measuring specific workload phases (reset between phases)
	/// - Repeated benchmark runs (reset between iterations)
	/// - Ignoring cold-start allocation costs (reset after warm-up)
	///
	/// # Example
	///
	/// ```ignore
	/// // Warm-up phase
	/// warm_up_queue(&queue);
	///
	/// // Reset stats to measure steady-state only
	/// queue.reset_allocation_stats();
	///
	/// // Run benchmark
	/// benchmark(&queue);
	///
	/// println!("Steady-state reuse rate: {:.1}%", queue.pool_reuse_rate());
	/// ```
	#[inline(always)]
	pub fn reset_allocation_stats(&self) {
		self.queue.reset_allocation_stats();
	}

	/// Returns the current number of allocated segments (live memory footprint).
	///
	/// See [`SegSpsc::allocated_segments()`](Spsc::allocated_segments) for details.
	#[inline(always)]
	pub fn allocated_segments(&self) -> u64 {
		self.queue.allocated_segments()
	}

	/// Calculates the current memory usage in bytes for allocated segments.
	///
	/// See [`SegSpsc::allocated_memory_bytes()`](Spsc::allocated_memory_bytes) for details.
	#[inline(always)]
	pub fn allocated_memory_bytes(&self) -> usize {
		self.queue.allocated_memory_bytes()
	}

	/// Closes the underlying queue immediately, waking the consumer.
	///
	/// This is exposed for internal integrations (e.g. async wrappers) that need to
	/// propagate shutdown without relying on `Drop`.
	#[inline(always)]
	pub(crate) fn close_channel(&self) {
		unsafe {
			self.queue.close();
		}
	}
}

impl<T, const P: usize, const NUM_SEGS_P2: usize> Drop for Sender<T, P, NUM_SEGS_P2> {
	fn drop(&mut self) {
		unsafe {
			self.queue.close();
		}
	}
}

/// **Consumer half of a segmented SPSC (single-producer, single-consumer) queue.**
///
/// This struct provides the dequeue API for the lock-free SPSC queue. It maintains
/// an `Arc` reference to the shared `SegSpsc` state and implements the consumer-side
/// operations: `try_pop`, `try_pop_n`, `consume_in_place`, and segment sealing logic.
///
/// # Design Philosophy
///
/// - **Split Ownership**: Producer and consumer are separate types to enforce
///   single-producer/single-consumer semantics at compile time
/// - **Arc-based**: Uses `Arc` for shared state, allowing drop semantics and
///   proper cleanup when either half is dropped
/// - **Cache-optimized**: Consumer state (tail, head_cache) is cache-padded
///   separately from producer state to prevent false sharing
/// - **Zero-copy option**: `consume_in_place()` allows processing items without copying
///
/// # Memory Model
///
/// The consumer owns the "tail" position (monotonically increasing u64):
/// ```text
/// Producer writes → [head, tail] → Consumer reads
///                    ↑queue data↑
/// ```
///
/// - **Tail**: Published via `Ordering::Release` after data reads
/// - **Head Cache**: Local copy of producer's head, refreshed with `Ordering::Acquire`
///
/// # Type Parameters
///
/// - `T`: Item type, must be `Copy` (allows lock-free memcpy semantics)
/// - `P`: Segment size exponent (segment_size = 2^P items)
/// - `NUM_SEGS_P2`: Number of segments exponent (num_segs = 2^NUM_SEGS_P2)
///
/// # Capacity
///
/// Total capacity: `(2^P × 2^NUM_SEGS_P2) - 1` items
///
/// Examples:
/// - `Receiver<u64, 10, 0>`: 1023 items (1024 - 1)
/// - `Receiver<u64, 13, 2>`: 32767 items (8192 × 4 - 1)
///
/// # Segment Sealing
///
/// When the consumer advances past a segment boundary, it "seals" the segment:
/// 1. **Checks if segment can be sealed**: `tail > sealable_lo + SEG_SIZE`
/// 2. **Stores segment pointer**: Push to sealed pool (lock-free stack)
/// 3. **Nulls directory entry**: Allows producer to reuse from pool
///
/// This enables memory reuse without heap allocation/deallocation churn.
///
/// # Zero-Copy Consumption
///
/// `consume_in_place()` provides direct access to queue items via callback:
/// ```ignore
/// consumer.consume_in_place(10, |items| {
///     // Process items without copying
///     for item in items {
///         process(*item);
///     }
///     items.len()  // Return number consumed
/// });
/// ```
///
/// # Close Semantics
///
/// The queue can be closed via `SegSpsc::close()`:
/// - Consumer can detect close state via `is_closed()`
/// - `try_pop_n()` returns `Err(PopError::Closed)` when empty + closed
/// - Consumer can still drain remaining items after close
///
/// # Thread Safety
///
/// - **Single Consumer**: Only ONE thread may hold the `Receiver`
/// - **Concurrent Producer**: Safe concurrent operation with the `Sender`
/// - **Send + !Sync**: Can be sent to another thread, but not shared
///
/// # Performance Characteristics
///
/// | Operation            | Latency (cache hit) | Latency (segment boundary) |
/// |----------------------|---------------------|----------------------------|
/// | `try_pop`            | ~5-15 ns            | ~20-40 ns                  |
/// | `try_pop_n`          | ~10-20 ns           | ~30-50 ns per boundary     |
/// | `consume_in_place`   | ~8-18 ns            | ~25-45 ns per boundary     |
/// | `len`                | ~2-5 ns             | N/A                        |
///
/// # Example
///
/// ```ignore
/// use bop_mpmc::SegSpsc;
///
/// type Queue = SegSpsc<u64, 10, 0>;  // 1023-item queue
/// let (producer, consumer) = Queue::new();
///
/// // Producer thread
/// std::thread::spawn(move || {
///     for i in 0..100 {
///         producer.try_push(i).unwrap();
///     }
/// });
///
/// // Consumer thread
/// std::thread::spawn(move || {
///     // Copy-based consumption
///     while let Some(item) = consumer.try_pop() {
///         println!("Got: {}", item);
///     }
///
///     // Zero-copy consumption
///     consumer.consume_in_place(16, |items| {
///         for item in items {
///             process(*item);
///         }
///         items.len()
///     });
/// });
/// ```
///
/// # See Also
///
/// - [`Sender`]: The producer half
/// - [`Spsc`]: The underlying shared queue structure
/// - [`PopError`]: Error types for pop operations
pub struct Receiver<T, const P: usize, const NUM_SEGS_P2: usize> {
	queue: Arc<Spsc<T, P, NUM_SEGS_P2>>,
}

impl<T, const P: usize, const NUM_SEGS_P2: usize> Receiver<T, P, NUM_SEGS_P2> {
	/// Returns the number of items currently in the queue.
	///
	/// This is calculated as `head - tail`, where both positions are monotonically
	/// increasing u64 counters. The close bit is masked out before subtraction.
	///
	/// # Close Bit Handling
	///
	/// The head position may have the close bit (bit 63) set. This method masks it out:
	/// ```ignore
	/// let h_pos = h & RIGHT_MASK;  // Clear bit 63
	/// let length = h_pos.saturating_sub(t);  // Prevent underflow
	/// ```
	///
	/// # Consistency
	///
	/// Uses `Ordering::Acquire` for both loads to ensure visibility of preceding writes.
	/// However, the value may be stale by the time the caller uses it (concurrent operations).
	///
	/// # Performance
	///
	/// ~2-5 ns (two atomic loads + subtraction)
	///
	/// # Example
	///
	/// ```ignore
	/// queue.try_push(1)?;
	/// queue.try_push(2)?;
	/// assert_eq!(queue.len(), 2);
	/// ```
	#[inline(always)]
	pub fn len(&self) -> usize {
		self.queue.len()
	}

	/// Returns `true` if the queue contains no items.
	///
	/// Equivalent to `self.len() == 0`, but more readable.
	///
	/// # Note
	///
	/// This check is not atomic with subsequent operations. A producer may enqueue
	/// items immediately after this returns `true`.
	///
	/// # Example
	///
	/// ```ignore
	/// assert!(queue.is_empty());
	/// queue.try_push(1)?;
	/// assert!(!queue.is_empty());
	/// ```
	#[inline(always)]
	pub fn is_empty(&self) -> bool {
		self.queue.is_empty()
	}

	/// Returns `true` if the queue is at maximum capacity.
	///
	/// When the queue is full, `try_push()` and `try_push_n()` will return `PushError::Full`.
	///
	/// # Capacity
	///
	/// The capacity is `(segment_size * num_segments) - 1`:
	/// - For `SegSpsc<T, 10, 0>`: `1024 - 1 = 1023` items
	/// - For `SegSpsc<T, 13, 2>`: `(8192 * 4) - 1 = 32767` items
	///
	/// # Note
	///
	/// Like `is_empty()`, this is not atomic with subsequent operations.
	///
	/// # Example
	///
	/// ```ignore
	/// for i in 0..queue.capacity() {
	///     queue.try_push(i).unwrap();
	/// }
	/// assert!(queue.is_full());
	/// assert!(queue.try_push(999).is_err());  // Returns PushError::Full
	/// ```
	#[inline(always)]
	pub fn is_full(&self) -> bool {
		self.queue.is_full()
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Close API
	// ──────────────────────────────────────────────────────────────────────────────

	/// Checks if the queue has been closed.
	///
	/// The close state is encoded in bit 63 of the head position, allowing lock-free
	/// detection without a separate atomic flag.
	///
	/// # Close Bit Encoding
	///
	/// ```text
	/// Head Position (64 bits):
	/// ┌─────────┬────────────────────────────────────────────────┐
	/// │ Bit 63  │ Bits 62..0                                     │
	/// │ (Close) │ (Monotonic Position)                           │
	/// └─────────┴────────────────────────────────────────────────┘
	/// ```
	///
	/// # Semantics
	///
	/// Once closed:
	/// - All `try_push()` and `try_push_n()` calls return `PushError::Closed`
	/// - Consumers can drain remaining items
	/// - `try_pop_n()` returns `PopError::Closed` when queue is empty
	///
	/// # Memory Ordering
	///
	/// Uses `Ordering::Relaxed` because close state doesn't need to synchronize with
	/// data operations (producers/consumers already use Acquire/Release for data visibility).
	///
	/// # Performance
	///
	/// ~1-2 ns (single atomic load + bit test)
	///
	/// # Example
	///
	/// ```ignore
	/// queue.try_push(1)?;
	/// queue.close();
	/// assert!(queue.is_closed());
	/// assert!(matches!(queue.try_push(2), Err(PushError::Closed(_))));
	/// assert_eq!(queue.try_pop(), Some(1));  // Can still drain
	/// ```
	#[inline(always)]
	pub fn is_closed(&self) -> bool {
		self.queue.is_closed()
	}

	/// Attempts to dequeue a single item by copying.
	///
	/// This is a convenience wrapper around `try_pop_n()` for single-item operations.
	/// The item is copied out of the queue into the returned `Option`.
	///
	/// # Returns
	///
	/// - `Some(item)`: Item successfully dequeued
	/// - `None`: Queue is empty or closed
	///
	/// # Distinguishing Empty vs Closed
	///
	/// This method returns `None` for both empty and closed states. Use `try_pop_n()` or
	/// check `is_closed()` separately if you need to distinguish:
	///
	/// ```ignore
	/// match queue.try_pop() {
	///     Some(item) => process(item),
	///     None if queue.is_closed() => break,  // Closed
	///     None => wait_for_items(),            // Empty
	/// }
	/// ```
	///
	/// # Head Caching
	///
	/// The consumer maintains a cached copy of the producer's head position to avoid
	/// unnecessary cache line bouncing. Only refreshes when the cached value shows empty.
	///
	/// # Performance
	///
	/// ~5-15 ns for successful pop (cache hit, no segment boundary crossing)
	///
	/// # Safety
	///
	/// Initializes return value with `mem::zeroed()`. Safe because:
	/// - If pop succeeds (returns `Some`), the zeroed value is overwritten
	/// - If pop fails (returns `None`), the zeroed value is discarded
	///
	/// # Example
	///
	/// ```ignore
	/// queue.try_push(42)?;
	/// assert_eq!(queue.try_pop(), Some(42));
	/// assert_eq!(queue.try_pop(), None);  // Empty
	/// ```
	#[inline(always)]
	pub fn try_pop(&self) -> Option<T> {
		self.queue.try_pop()
	}

	/// Attempts to dequeue multiple items by copying into the destination slice.
	///
	/// Copies items from the queue into `dst`, potentially spanning multiple segments.
	/// Returns the number of items successfully copied (may be partial if queue empties).
	///
	/// # Algorithm
	///
	/// 1. **Availability Check**: Calculate available items using cached head (refresh if needed)
	/// 2. **Close Check**: Return `Err(PopError::Closed)` if empty and bit 63 is set
	/// 3. **Bulk Copy**: Copy min(dst.len(), avail) items across segment boundaries
	/// 4. **Tail Update**: Atomically publish new tail position with Release ordering
	/// 5. **Segment Sealing**: Check if segments can be sealed and returned to pool
	///
	/// # Head Caching
	///
	/// The consumer maintains a cached copy of the producer's head position:
	/// - **Fast path**: If cached head shows available items, skip atomic load
	/// - **Slow path**: If cached head shows empty, refresh from producer's atomic head
	///
	/// This optimization eliminates cache line bouncing in the common case where the
	/// queue has available items.
	///
	/// # Close Bit Handling
	///
	/// The head position may have bit 63 set to indicate closure:
	/// ```ignore
	/// let h_pos = h & RIGHT_MASK;  // Mask out close bit
	/// let avail = h_pos.saturating_sub(t);
	/// if avail == 0 && (h & CLOSED_CHANNEL_MASK != 0) {
	///     return Err(PopError::Closed);
	/// }
	/// ```
	///
	/// # Segment Boundaries
	///
	/// Items may span multiple segments. For each segment:
	/// ```ignore
	/// seg_idx = (position >> P) & DIR_MASK;  // Which segment in directory
	/// offset = position & SEG_MASK;          // Offset within segment
	/// ```
	///
	/// After consumption, segments in the range [sealable_lo, sealable_hi) are marked
	/// as sealed and available for reuse via the pooling mechanism.
	///
	/// # Memory Ordering
	///
	/// - Head cache: `Acquire` (when refreshing from producer)
	/// - Segment load: `Acquire` (ensures data visibility)
	/// - Tail update: `Release` (publishes consumed position to producer)
	///
	/// # Returns
	///
	/// - `Ok(n)`: `n` items successfully dequeued (0 ≤ n ≤ dst.len())
	/// - `Err(PopError::Empty)`: Queue is empty
	/// - `Err(PopError::Closed)`: Queue is closed and empty
	///
	/// # Partial Success
	///
	/// If `dst.len() > avail`, only `avail` items are copied and `Ok(avail)` is returned:
	///
	/// ```ignore
	/// let mut buffer = vec![0; 100];
	/// match queue.try_pop_n(&mut buffer) {
	///     Ok(n) => println!("Dequeued {} items", n),
	///     Err(PopError::Empty) => println!("Queue empty"),
	///     Err(PopError::Closed) => println!("Queue closed"),
	/// }
	/// ```
	///
	/// # Performance
	///
	/// - **Single segment**: ~10-20 ns (one memcpy)
	/// - **Multiple segments**: ~20-30 ns per segment boundary (memcpy + sealing check)
	/// - **Throughput**: ~500M-1B ops/sec on modern x86_64 (depending on item size)
	///
	/// # Safety
	///
	/// Uses `ptr::copy_nonoverlapping` internally. Safety invariants:
	/// - Source segment is allocated and contains initialized items
	/// - Destination slice is valid for writes
	/// - No overlap between source and destination (guaranteed by design)
	/// - Items are `Copy`, so no double-drop concerns
	#[inline(always)]
	pub fn try_pop_n(&self, dst: &mut [T]) -> Result<usize, PopError> {
		self.queue.try_pop_n(dst)
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Consumer API (ZERO-COPY, process-in-place)
	// ──────────────────────────────────────────────────────────────────────────────

	/// Zero-copy consumption: processes items in-place via callback with read-only slices.
	///
	/// This method provides direct read access to items in the queue without copying.
	/// The callback `f` is invoked with contiguous slices of items (up to segment boundaries),
	/// and must return the number of items it actually consumed.
	///
	/// # Zero-Copy Design
	///
	/// Unlike `try_pop_n()` which copies items into a destination buffer, this method:
	/// - Exposes items directly in their segment storage (zero memory copies)
	/// - Allows processing without intermediate allocation
	/// - Ideal for serialization, hashing, or forwarding to I/O
	///
	/// # Algorithm
	///
	/// 1. **Availability Check**: Calculate available items using cached head
	/// 2. **Segment Iteration**: For each segment in [tail, tail + max):
	///    - Calculate contiguous slice within segment boundary
	///    - Invoke callback with read-only slice
	///    - Advance tail by the number of items callback consumed
	///    - Seal segment if boundary crossed
	/// 3. **Early Exit**: Stop if callback returns 0 (processed fewer than offered)
	///
	/// # Callback Contract
	///
	/// ```ignore
	/// fn callback(slice: &[T]) -> usize
	/// ```
	///
	/// - **Input**: Read-only slice of contiguous items (may be less than requested due to segment boundaries)
	/// - **Output**: Number of items consumed (0 ≤ consumed ≤ slice.len())
	/// - **Lifetime**: Slice MUST NOT be held beyond the callback scope (undefined behavior)
	/// - **Early Exit**: Return < slice.len() to stop iteration
	///
	/// # Segment Boundaries
	///
	/// Items are presented in segment-sized chunks. If you request 10,000 items but segment
	/// size is 1024, the callback will be invoked multiple times:
	/// - 1st call: slice of up to 1024 items (or fewer if at segment boundary)
	/// - 2nd call: next slice of up to 1024 items
	/// - ... (continues until `max` items processed or callback returns 0)
	///
	/// # Close Semantics
	///
	/// This method does NOT check the close bit or return an error. It simply processes
	/// whatever items are available. Use `is_closed()` separately if needed:
	///
	/// ```ignore
	/// loop {
	///     let consumed = queue.consume_in_place(1024, |slice| {
	///         serialize(slice);
	///         slice.len()  // Consumed all
	///     });
	///
	///     if consumed == 0 {
	///         if queue.is_closed() {
	///             break;  // Done
	///         }
	///         wait_for_items();
	///     }
	/// }
	/// ```
	///
	/// # Parameters
	///
	/// - `max`: Maximum number of items to consume (may consume fewer if queue empties or callback exits early)
	/// - `f`: Callback invoked with read-only slices, returning number of items consumed
	///
	/// # Returns
	///
	/// Total number of items consumed across all callback invocations.
	///
	/// # Examples
	///
	/// **Example 1: Serialize to network without copying**
	/// ```ignore
	/// let bytes_sent = queue.consume_in_place(batch_size, |items| {
	///     match socket.write_all(bytemuck::cast_slice(items)) {
	///         Ok(()) => items.len(),     // Consumed all
	///         Err(_) => 0,               // Error, stop
	///     }
	/// });
	/// ```
	///
	/// **Example 2: Compute hash without allocation**
	/// ```ignore
	/// let mut hasher = Blake3::new();
	/// queue.consume_in_place(usize::MAX, |items| {
	///     hasher.update(bytemuck::cast_slice(items));
	///     items.len()  // Consumed all
	/// });
	/// let hash = hasher.finalize();
	/// ```
	///
	/// **Example 3: Conditional processing with early exit**
	/// ```ignore
	/// let mut count = 0;
	/// queue.consume_in_place(1000, |items| {
	///     for (i, item) in items.iter().enumerate() {
	///         if !should_process(item) {
	///             return i;  // Stop here
	///         }
	///         process(item);
	///         count += 1;
	///     }
	///     items.len()  // Consumed all
	/// });
	/// ```
	///
	/// # Performance
	///
	/// - **Throughput**: 2-5 GB/s for simple processing (memory bandwidth limited)
	/// - **Latency**: ~5-10 ns per callback invocation overhead
	/// - **Advantage over copying**: Saves ~50-200 ns per 1KB of data (avoids memcpy)
	///
	/// # Safety
	///
	/// This method is safe because:
	/// - Items are `Copy`, so no drop concerns
	/// - Producer can't modify consumed items (monotonic head/tail invariant)
	/// - Consumer is single-threaded (SPSC guarantee)
	/// - Slices are bounded to initialized region [tail, head)
	///
	/// **CRITICAL**: The callback MUST NOT store the slice reference beyond its scope.
	/// The slice becomes invalid after the tail is updated.
	#[inline(always)]
	pub fn consume_in_place<F>(&self, max: usize, f: F) -> usize
	where
			F: FnMut(&[T]) -> usize,
	{
		self.queue.consume_in_place(max, f)
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Metrics API
	// ──────────────────────────────────────────────────────────────────────────────

	/// Returns the number of fresh segment allocations (pool misses).
	///
	/// This metric tracks how many times `ensure_segment_for()` had to allocate new
	/// memory from the system allocator because no sealed segment was available for reuse.
	///
	/// # Interpretation
	///
	/// - **High value**: Indicates working set exceeds pooled segment capacity (more allocations)
	/// - **Low value**: Indicates effective pooling (segments are being reused efficiently)
	///
	/// # Use Cases
	///
	/// - **Performance tuning**: Compare fresh vs reused to assess pool effectiveness
	/// - **Capacity planning**: High fresh count may indicate need for larger `NUM_SEGS_P2`
	/// - **Memory profiling**: Track allocation patterns over time
	///
	/// # Example
	///
	/// ```ignore
	/// queue.reset_allocation_stats();
	/// // ... run workload ...
	/// println!("Fresh allocations: {}", queue.fresh_allocations());
	/// println!("Pool reuse rate: {:.1}%", queue.pool_reuse_rate());
	/// ```
	#[inline(always)]
	pub fn fresh_allocations(&self) -> u64 {
		self.queue.fresh_allocations()
	}

	/// Returns the number of segment pool reuses (pool hits).
	///
	/// This metric tracks how many times `ensure_segment_for()` successfully reused a
	/// sealed segment from the pool instead of allocating new memory.
	///
	/// # Interpretation
	///
	/// - **High value**: Indicates effective pooling and good cache locality
	/// - **Low value**: May indicate insufficient sealed segments or high churn rate
	///
	/// # Pool Reuse Mechanism
	///
	/// Segments in the range [sealable_lo, sealable_hi) are available for reuse.
	/// When the producer needs a new segment, it first checks this range. If available,
	/// the segment is moved (not copied) to the target slot in the directory.
	///
	/// # Example
	///
	/// ```ignore
	/// let before = queue.pool_reuses();
	/// // ... enqueue/dequeue cycle that triggers segment reuse ...
	/// let after = queue.pool_reuses();
	/// println!("Segments reused: {}", after - before);
	/// ```
	#[inline(always)]
	pub fn pool_reuses(&self) -> u64 {
		self.queue.pool_reuses()
	}

	/// Returns the segment pool reuse rate as a percentage (0-100%).
	///
	/// Calculated as: `(pool_reuses / (fresh_allocations + pool_reuses)) * 100`
	///
	/// # Interpretation
	///
	/// - **0%**: No reuse (all allocations are fresh) - may indicate cold start or insufficient pool
	/// - **50%**: Half of allocations are reused - moderate pooling effectiveness
	/// - **90%+**: Excellent reuse - pool is working well, high cache locality
	///
	/// # Performance Impact
	///
	/// - **Fresh allocation**: ~100-500 ns (system allocator + initialization)
	/// - **Pool reuse**: ~5-10 ns (pointer swap)
	///
	/// High reuse rates directly translate to lower latency and higher throughput.
	///
	/// # Example
	///
	/// ```ignore
	/// // Warm up the pool
	/// for _ in 0..queue.capacity() {
	///     queue.try_push(0)?;
	/// }
	/// for _ in 0..queue.capacity() {
	///     queue.try_pop();
	/// }
	///
	/// queue.reset_allocation_stats();
	///
	/// // Run steady-state workload
	/// for _ in 0..1_000_000 {
	///     queue.try_push(42)?;
	///     queue.try_pop();
	/// }
	///
	/// println!("Reuse rate: {:.1}%", queue.pool_reuse_rate());
	/// // Expected: ~99% after warm-up phase
	/// ```
	#[inline(always)]
	pub fn pool_reuse_rate(&self) -> f64 {
		self.queue.pool_reuse_rate()
	}

	/// Resets allocation statistics to zero.
	///
	/// Useful for:
	/// - Measuring specific workload phases (reset between phases)
	/// - Repeated benchmark runs (reset between iterations)
	/// - Ignoring cold-start allocation costs (reset after warm-up)
	///
	/// # Example
	///
	/// ```ignore
	/// // Warm-up phase
	/// warm_up_queue(&queue);
	///
	/// // Reset stats to measure steady-state only
	/// queue.reset_allocation_stats();
	///
	/// // Run benchmark
	/// benchmark(&queue);
	///
	/// println!("Steady-state reuse rate: {:.1}%", queue.pool_reuse_rate());
	/// ```
	#[inline(always)]
	pub fn reset_allocation_stats(&self) {
		self.queue.reset_allocation_stats();
	}

	/// Returns the current number of allocated segments (live memory footprint).
	///
	/// See [`SegSpsc::allocated_segments()`](Spsc::allocated_segments) for details.
	#[inline(always)]
	pub fn allocated_segments(&self) -> u64 {
		self.queue.allocated_segments()
	}

	/// Calculates the current memory usage in bytes for allocated segments.
	///
	/// See [`SegSpsc::allocated_memory_bytes()`](Spsc::allocated_memory_bytes) for details.
	#[inline(always)]
	pub fn allocated_memory_bytes(&self) -> usize {
		self.queue.allocated_memory_bytes()
	}

	/// Attempts to deallocate excess segments from the sealed pool.
	///
	/// See [`SegSpsc::try_deallocate_excess()`](Spsc::try_deallocate_excess) for details.
	#[inline(always)]
	pub fn try_deallocate_excess(&self) -> u64 {
		self.queue.try_deallocate_excess()
	}

	/// Deallocates sealed pool segments down to a specific target size.
	///
	/// See [`SegSpsc::deallocate_to()`](Spsc::deallocate_to) for details.
	#[inline(always)]
	pub fn deallocate_to(&self, target_pool_size: usize) -> u64 {
		self.queue.deallocate_to(target_pool_size)
	}
}

/// A bounded, segmented SPSC queue with lazy allocation, segment pooling, and zero-copy APIs.
///
/// # Type Parameters
///
/// - `T`: Item type (must be `Copy` for efficient memcpy and no Drop concerns)
/// - `P`: log2(segment_size) - Each segment holds `2^P` items
/// - `NUM_SEGS_P2`: log2(directory_size) - Directory has `2^NUM_SEGS_P2` slots
///
/// # Capacity
///
/// Total capacity = `(2^P × 2^NUM_SEGS_P2) - 1` (one-empty-slot rule)
///
/// Example: `SegSpsc<u64, 8, 10>` = 256 items/seg × 1024 segs = 262,143 capacity
///
/// # Memory Layout
///
/// The queue consists of three main parts:
///
/// 1. **Segment directory**: Fixed array of `AtomicPtr` (NUM_SEGS slots)
/// 2. **Producer state**: Cache-padded head, tail_cache, sealable_lo, metrics
/// 3. **Consumer state**: Cache-padded tail, head_cache, sealable_hi
///
/// Segments are allocated lazily and pooled for reuse. This minimizes:
/// - Initial memory footprint (no upfront allocation)
/// - Allocator pressure (reuse hot segments)
/// - Cache misses (pool retains recently-used memory)
///
/// # Thread Safety
///
/// This is a **single-producer, single-consumer** queue:
/// - Exactly **one** thread may call producer methods (`try_push`, `try_push_n`)
/// - Exactly **one** thread may call consumer methods (`try_pop`, `try_pop_n`, `consume_in_place`)
/// - Simultaneous calls to inspection methods (`len`, `is_empty`) are safe but may see stale values
///
/// Violating these constraints causes **undefined behavior** (data races on UnsafeCells).
///
/// # Close Semantics
///
/// The queue can be closed via `close()`, which sets bit 63 of the head position:
/// - Producers see `PushError::Closed` on subsequent pushes
/// - Consumers can drain remaining items, then see `PopError::Closed` when empty
/// - Close is **one-way** (cannot reopen)
/// - Close is **lock-free** (encoded in head position)
///
/// # Examples
///
/// ## Basic Push/Pop
///
/// ```ignore
/// use bop_mpmc::seg_spsc::SegSpsc;
///
/// type Queue = SegSpsc<u64, 6, 8>;  // 64 items/seg, 256 segs
/// let q = Queue::new();
///
/// q.try_push(42).unwrap();
/// assert_eq!(q.try_pop(), Some(42));
/// ```
///
/// ## Bulk Operations
///
/// ```ignore
/// let items = vec![1, 2, 3, 4, 5];
/// q.try_push_n(&items).unwrap();
///
/// let mut buf = vec![0; 5];
/// q.try_pop_n(&mut buf).unwrap();
/// assert_eq!(buf, items);
/// ```
///
/// ## Zero-Copy Consumption
///
/// ```ignore
/// q.try_push_n(&[10, 20, 30, 40]).unwrap();
///
/// let total = q.consume_in_place(100, |slice| {
///     for &item in slice {
///         println!("{}", item);
///     }
///     slice.len()  // Consume all items in this slice
/// });
/// assert_eq!(total, 4);
/// ```
///
/// ## Graceful Shutdown
///
/// ```ignore
/// // Producer closes queue
/// q.close();
///
/// // Consumer drains remaining items
/// while let Some(item) = q.try_pop() {
///     process(item);
/// }
/// // Now sees PopError::Closed
/// ```
pub struct Spsc<T, const P: usize, const NUM_SEGS_P2: usize> {
	/// **Segment directory**: Fixed array of `AtomicPtr<MaybeUninit<T>>` slots.
	///
	/// Each slot either:
	/// - Points to an allocated segment (SEG_SIZE items)
	/// - Is null (segment not yet allocated or currently in sealed pool)
	///
	/// Producer lazily allocates segments on write boundaries.
	segs: Box<[AtomicPtr<MaybeUninit<T>>]>,

	/// **Producer state**: Cache-padded to avoid false sharing with consumer.
	producer: CachePadded<ProducerState>,

	/// **Consumer state**: Cache-padded to avoid false sharing with producer.
	consumer: CachePadded<ConsumerState>,

	/// **Optional signal gate**: For integration with `SignalWaker` (MPMC use case).
	///
	/// If present, `schedule()` is called after pushes to wake consumer threads.
	signal: CachePadded<Option<SignalGate>>,

	/// **Target pooled segments**: Target number of segments to maintain in sealed pool.
	///
	/// Maximum number of segments to keep in the sealed pool.
	/// When the sealed pool [sealable_lo, sealable_hi) exceeds this limit,
	/// excess segments are deallocated down TO this target.
	/// - `0` = deallocate all pooled segments (maximum memory reclamation)
	/// - `N` = keep N segments pooled (warm pool for reuse)
	/// - `usize::MAX` = disable deallocation (unbounded pool)
	max_pooled_segments: usize,
}

impl<T, const P: usize, const NUM_SEGS_P2: usize> Spsc<T, P, NUM_SEGS_P2> {
	/// Items per segment (2^P)
	pub const SEG_SIZE: usize = 1usize << P;

	/// Number of directory slots (2^NUM_SEGS_P2)
	pub const NUM_SEGS: usize = 1usize << NUM_SEGS_P2;

	/// Mask to extract offset within a segment
	pub const SEG_MASK: usize = Self::SEG_SIZE - 1;

	/// Mask to extract segment index from directory
	pub const DIR_MASK: usize = Self::NUM_SEGS - 1;

	/// Bit mask for the close flag in the head position (bit 63).
	///
	/// When this bit is set in `producer.head`, the queue is closed.
	const CLOSED_CHANNEL_MASK: u64 = 1u64 << 63;

	/// Bit mask covering all position bits (excludes close bit).
	///
	/// Used to extract the actual position from head: `head & RIGHT_MASK`
	const RIGHT_MASK: u64 = !Self::CLOSED_CHANNEL_MASK;

	/// Creates a new segmented SPSC queue, returning producer and consumer halves.
	///
	/// This is the primary constructor for safe queue creation. It allocates the segment
	/// directory but defers segment allocation until the producer writes to them (lazy allocation).
	///
	/// # Type Parameters
	///
	/// - `T`: Item type, must be `Copy` (enables lock-free memcpy)
	/// - `P`: Segment size exponent (segment_size = 2^P items)
	/// - `NUM_SEGS_P2`: Number of segments exponent (num_segs = 2^NUM_SEGS_P2)
	///
	/// # Capacity
	///
	/// The queue capacity is `(2^P × 2^NUM_SEGS_P2) - 1` items:
	/// ```ignore
	/// type SmallQueue = SegSpsc<u64, 10, 0>;  // 1023 items (2^10 × 2^0 - 1)
	/// type LargeQueue = SegSpsc<u64, 13, 2>;  // 32767 items (2^13 × 2^2 - 1)
	/// ```
	///
	/// # Memory Usage
	///
	/// **Initial allocation**:
	/// - Directory: `NUM_SEGS × 8 bytes` (AtomicPtr slots)
	/// - Producer state: ~128 bytes (cache-padded)
	/// - Consumer state: ~128 bytes (cache-padded)
	///
	/// **Per-segment allocation** (lazy):
	/// - Each segment: `SEG_SIZE × sizeof(T)` bytes
	///
	/// Example for `SegSpsc<u64, 10, 2>`:
	/// - Initial: `4 × 8 + 256 = 288 bytes`
	/// - Max (4 segments): `288 + 4 × 1024 × 8 = 33,056 bytes`
	///
	/// # No Signal Integration
	///
	/// This constructor does not attach a `SignalGate`. For integration with `SignalWaker`,
	/// use [`new_with_gate()`](Self::new_with_gate) instead.
	///
	/// # Thread Safety
	///
	/// The returned `Sender` and `Receiver` can be sent to different threads:
	/// ```ignore
	/// let (producer, consumer) = SegSpsc::<u64, 10, 0>::new();
	///
	/// let prod_thread = std::thread::spawn(move || {
	///     producer.try_push(42).unwrap();
	/// });
	///
	/// let cons_thread = std::thread::spawn(move || {
	///     assert_eq!(consumer.try_pop(), Some(42));
	/// });
	/// ```
	///
	/// # Performance Notes
	///
	/// - **Initial cost**: ~100-200 ns (directory allocation)
	/// - **Segment allocation**: ~50-100 ns per segment (amortized across SEG_SIZE pushes)
	/// - **Steady state**: ~5-15 ns per push/pop (cache-friendly operation)
	///
	/// # Example
	///
	/// ```ignore
	/// use bop_mpmc::SegSpsc;
	///
	/// // Create a 1023-item queue for u64 values
	/// type MyQueue = SegSpsc<u64, 10, 0>;
	/// let (producer, consumer) = MyQueue::new();
	///
	/// // Enqueue some items
	/// producer.try_push(1)?;
	/// producer.try_push(2)?;
	/// producer.try_push(3)?;
	///
	/// // Dequeue items
	/// assert_eq!(consumer.try_pop(), Some(1));
	/// assert_eq!(consumer.try_pop(), Some(2));
	/// assert_eq!(consumer.try_pop(), Some(3));
	/// assert_eq!(consumer.try_pop(), None);
	/// ```
	///
	/// # See Also
	///
	/// - [`new_with_gate()`](Self::new_with_gate): Constructor with signal integration
	/// - [`new_unsafe()`](Self::new_unsafe): Unsafe constructor (internal use)
	/// - [`capacity()`](Self::capacity): Returns maximum capacity
	pub fn new() -> (Sender<T, P, NUM_SEGS_P2>, Receiver<T, P, NUM_SEGS_P2>) {
		Self::new_with_config(usize::MAX) // Unbounded pool (deallocation disabled)
	}

	/// Creates a new segmented SPSC queue with configurable max pooled segments.
	///
	/// # Arguments
	///
	/// * `max_pooled_segments` - Maximum number of segments to keep in the sealed pool.
	///   When the sealed pool [sealable_lo, sealable_hi) exceeds this limit, the consumer
	///   deallocates the oldest segments during boundary crossing to limit memory usage.
	///
	///   - `0` = deallocate all pooled segments (maximum memory reclamation)
	///   - `N` = keep N segments pooled (recommended: 4-16 for most workloads)
	///   - `usize::MAX` = disable deallocation (unbounded pool)
	///
	/// # Memory Management Strategy
	///
	/// During bursty workloads, the producer may allocate all NUM_SEGS segments. This
	/// configuration allows the consumer to gradually deallocate segments that fall
	/// outside the active working set, bringing memory usage back down after the burst.
	///
	/// # Example
	///
	/// ```ignore
	/// // Limit to 8 pooled segments (deallocate excess)
	/// let (producer, consumer) = SegSpsc::<u64, 10, 4>::new_with_config(8);
	/// ```
	pub fn new_with_config(
		max_pooled_segments: usize,
	) -> (Sender<T, P, NUM_SEGS_P2>, Receiver<T, P, NUM_SEGS_P2>) {
		let queue = Arc::new(unsafe { Self::new_unsafe_with_config(max_pooled_segments) });
		(
			Sender {
				queue: Arc::clone(&queue),
			},
			Receiver {
				queue: Arc::clone(&queue),
			},
		)
	}

	/// Creates a new segmented SPSC queue with signal integration, returning producer and consumer halves.
	///
	/// This constructor attaches a `SignalGate` to the queue, enabling automatic signaling
	/// when items are enqueued. After each successful push, the gate is scheduled via
	/// `SignalGate::schedule()`, which notifies the associated `SignalWaker` to wake
	/// executor threads.
	///
	/// # Use Cases
	///
	/// This constructor is intended for integration with the MPMC executor system:
	/// - **Async task queues**: Wake executor threads when tasks are enqueued
	/// - **Work-stealing**: Signal idle workers that work is available
	/// - **Event loops**: Notify event loop threads of new events
	///
	/// For simple SPSC usage without executor integration, use [`new()`](Self::new) instead.
	///
	/// # Arguments
	///
	/// * `gate` - The `SignalGate` to schedule after each push
	///
	/// # Signal Protocol
	///
	/// The signal protocol involves three components:
	/// 1. **SignalGate**: Per-queue state (bit index in 64-bit signal word)
	/// 2. **SignalWord**: 64-bit atomic bitmap (one bit per queue in the group)
	/// 3. **SignalWaker**: Global waker with summary bitmap (one bit per signal word)
	///
	/// When the producer calls `try_push()`:
	/// ```ignore
	/// queue.try_push(item)?;
	/// // Internally calls: queue.schedule()
	/// // Which calls: gate.schedule()
	/// // Which sets: signal_word |= (1 << bit_index)
	/// // And potentially: waker.summary |= (1 << group_index)
	/// ```
	///
	/// # Memory Ordering
	///
	/// The signal protocol uses `Ordering::AcqRel` to ensure:
	/// - Push data is visible before the signal bit is set
	/// - Consumer sees the signal bit before reading data
	///
	/// # Performance Impact
	///
	/// Adding signal integration adds ~2-5 ns overhead per push (one atomic RMW operation).
	///
	/// # Example
	///
	/// ```ignore
	/// use bop_mpmc::{SignalGate, SignalWaker, SegSpsc};
	/// use std::sync::Arc;
	///
	/// // Create shared waker
	/// let waker = Arc::new(SignalWaker::new());
	///
	/// // Create signal word and gate
	/// let signal_word = Arc::new(AtomicU64::new(0));
	/// let gate = SignalGate::new(0, Arc::clone(&signal_word), Arc::clone(&waker));
	///
	/// // Create queue with gate
	/// let (producer, consumer) = SegSpsc::<u64, 10, 0>::new_with_gate(gate);
	///
	/// // Producer thread
	/// std::thread::spawn(move || {
	///     producer.try_push(42).unwrap();
	///     // Automatically calls gate.schedule()
	/// });
	///
	/// // Executor thread (simplified)
	/// std::thread::spawn(move || {
	///     loop {
	///         if signal_word.load(Ordering::Acquire) & 1 != 0 {
	///             // Process item
	///             if let Some(item) = consumer.try_pop() {
	///                 process(item);
	///             }
	///         }
	///     }
	/// });
	/// ```
	///
	/// # See Also
	///
	/// - [`new()`](Self::new): Constructor without signal integration
	/// - [`new_unsafe_with_gate()`](Self::new_unsafe_with_gate): Unsafe constructor with gate
	/// - [`schedule()`](Self::schedule): Manually schedule the gate
	/// - `SignalGate`: Per-queue signal state
	/// - `SignalWaker`: Global executor waker
	pub fn new_with_gate(
		gate: SignalGate,
	) -> (Sender<T, P, NUM_SEGS_P2>, Receiver<T, P, NUM_SEGS_P2>) {
		Self::new_with_gate_and_config(gate, 0)
	}

	/// Creates a new segmented SPSC queue with signal gate and configurable max pooled segments.
	///
	/// Combines signal integration with memory management configuration.
	///
	/// # Arguments
	///
	/// * `gate` - The signal gate for executor integration
	/// * `max_pooled_segments` - Maximum number of segments to keep in the sealed pool.
	///   - `0` = deallocate all pooled segments
	///   - `N` = keep N segments pooled
	///   - `usize::MAX` = disable deallocation (unbounded pool)
	///
	/// # Example
	///
	/// ```ignore
	/// let gate = SignalGate::new(0, signal_word, waker);
	/// let (producer, consumer) = SegSpsc::<u64, 10, 4>::new_with_gate_and_config(gate, 8);
	/// ```
	pub fn new_with_gate_and_config(
		gate: SignalGate,
		max_pooled_segments: usize,
	) -> (Sender<T, P, NUM_SEGS_P2>, Receiver<T, P, NUM_SEGS_P2>) {
		let queue =
				Arc::new(unsafe { Self::new_unsafe_with_gate_and_config(gate, max_pooled_segments) });
		(
			Sender {
				queue: Arc::clone(&queue),
			},
			Receiver {
				queue: Arc::clone(&queue),
			},
		)
	}

	/// Creates a new empty queue with all segments unallocated (unsafe, internal use).
	///
	/// This is the low-level constructor called by [`new()`](Self::new). It is marked
	/// `unsafe` because it returns a raw `Self` value that must be immediately wrapped
	/// in an `Arc` to maintain the SPSC safety guarantees.
	///
	/// # Safety
	///
	/// Callers must ensure:
	/// 1. The returned value is immediately wrapped in `Arc<Self>`
	/// 2. Only one `Sender` is created from the Arc (single producer)
	/// 3. Only one `Receiver` is created from the Arc (single consumer)
	///
	/// The `new()` and `new_with_gate()` constructors handle this correctly.
	///
	/// # Memory Layout
	///
	/// The segment directory is allocated (NUM_SEGS slots), but individual segments
	/// are **not** allocated until the producer writes to them. This minimizes
	/// initial memory footprint.
	///
	/// **Initial allocation**:
	/// - Directory: `NUM_SEGS × 8 bytes` (AtomicPtr slots, all null)
	/// - Producer state: ~128 bytes (cache-padded, head=0, tail_cache=0)
	/// - Consumer state: ~128 bytes (cache-padded, tail=0, head_cache=0)
	///
	/// **Segment allocation** (lazy, on first write to segment):
	/// - Per segment: `SEG_SIZE × sizeof(T)` bytes
	///
	/// # No Signal Integration
	///
	/// This constructor does not attach a `SignalGate`. Use [`new_unsafe_with_gate()`](Self::new_unsafe_with_gate)
	/// for signal integration.
	///
	/// # Initialization State
	///
	/// All atomic counters are initialized to zero:
	/// ```ignore
	/// producer.head = 0
	/// producer.tail_cache = 0
	/// producer.sealable_lo = 0
	/// producer.fresh_allocations = 0
	/// producer.pool_reuses = 0
	///
	/// consumer.tail = 0
	/// consumer.head_cache = 0
	/// consumer.sealable_hi = null
	/// ```
	///
	/// # Example
	///
	/// ```ignore
	/// // Internal usage (wrapped in Arc by new())
	/// let queue = Arc::new(unsafe { SegSpsc::<u64, 10, 0>::new_unsafe() });
	/// let producer = Sender { queue: Arc::clone(&queue) };
	/// let consumer = Receiver { queue: Arc::clone(&queue) };
	/// ```
	///
	/// # See Also
	///
	/// - [`new()`](Self::new): Safe public constructor
	/// - [`new_unsafe_with_gate()`](Self::new_unsafe_with_gate): Unsafe constructor with signal
	pub unsafe fn new_unsafe() -> Self {
		unsafe { Self::new_unsafe_with_config(0) }
	}

	/// Creates a new empty queue with configurable max pooled segments (unsafe, internal use).
	///
	/// # Arguments
	///
	/// * `max_pooled_segments` - Maximum number of segments to keep in the sealed pool.
	///   - `0` = deallocate all pooled segments
	///   - `N` = keep N segments pooled
	///   - `usize::MAX` = disable deallocation (unbounded pool)
	///
	/// # Safety
	///
	/// Same safety requirements as `new_unsafe()`.
	pub unsafe fn new_unsafe_with_config(max_pooled_segments: usize) -> Self {
		let mut v = Vec::with_capacity(Self::NUM_SEGS);
		for _ in 0..Self::NUM_SEGS {
			v.push(AtomicPtr::new(ptr::null_mut()));
		}

		Self {
			segs: v.into_boxed_slice(),
			producer: CachePadded::new(ProducerState::new()),
			consumer: CachePadded::new(ConsumerState::new()),
			signal: CachePadded::new(None),
			max_pooled_segments,
		}
	}

	/// Creates a new empty queue with signal integration (unsafe, internal use).
	///
	/// This is the low-level constructor called by [`new_with_gate()`](Self::new_with_gate).
	/// It is marked `unsafe` because it returns a raw `Self` value that must be immediately
	/// wrapped in an `Arc` to maintain the SPSC safety guarantees.
	///
	/// # Safety
	///
	/// Callers must ensure:
	/// 1. The returned value is immediately wrapped in `Arc<Self>`
	/// 2. Only one `Sender` is created from the Arc (single producer)
	/// 3. Only one `Receiver` is created from the Arc (single consumer)
	/// 4. The `signal` gate is properly configured with valid signal word and waker references
	///
	/// The `new_with_gate()` constructor handles this correctly.
	///
	/// # Arguments
	///
	/// * `signal` - The `SignalGate` to schedule after each push
	///
	/// # Signal Integration
	///
	/// The provided `SignalGate` is stored in `self.signal` and automatically invoked
	/// after each successful push operation:
	/// ```ignore
	/// queue.try_push(item)?;
	/// // Internally:
	/// if let Some(gate) = &self.signal {
	///     gate.schedule();  // Sets bit in signal word, wakes executor
	/// }
	/// ```
	///
	/// # Memory Layout
	///
	/// Identical to [`new_unsafe()`](Self::new_unsafe), plus:
	/// - Signal gate: ~128 bytes (cache-padded, contains Arc refs to signal word and waker)
	///
	/// **Total initial allocation**:
	/// - Directory: `NUM_SEGS × 8 bytes`
	/// - Producer state: ~128 bytes
	/// - Consumer state: ~128 bytes
	/// - Signal gate: ~128 bytes
	/// - **Total**: ~400-500 bytes (before segment allocation)
	///
	/// # Initialization State
	///
	/// All atomic counters are initialized to zero (same as `new_unsafe()`), plus:
	/// ```ignore
	/// signal = Some(gate)  // Wrapped in CachePadded
	/// ```
	///
	/// # Example
	///
	/// ```ignore
	/// use bop_mpmc::{SignalGate, SignalWaker, SegSpsc};
	/// use std::sync::Arc;
	///
	/// let waker = Arc::new(SignalWaker::new());
	/// let signal_word = Arc::new(AtomicU64::new(0));
	/// let gate = SignalGate::new(0, Arc::clone(&signal_word), Arc::clone(&waker));
	///
	/// // Internal usage (wrapped in Arc by new_with_gate())
	/// let queue = Arc::new(unsafe { SegSpsc::<u64, 10, 0>::new_unsafe_with_gate(gate) });
	/// let producer = Sender { queue: Arc::clone(&queue) };
	/// let consumer = Receiver { queue: Arc::clone(&queue) };
	/// ```
	///
	/// # See Also
	///
	/// - [`new_with_gate()`](Self::new_with_gate): Safe public constructor with signal
	/// - [`new_unsafe()`](Self::new_unsafe): Unsafe constructor without signal
	/// - [`schedule()`](Self::schedule): Manually invoke the signal gate
	pub unsafe fn new_unsafe_with_gate(signal: SignalGate) -> Self {
		unsafe { Self::new_unsafe_with_gate_and_config(signal, 0) }
	}

	/// Creates a new empty queue with signal gate and configurable max pooled segments (unsafe, internal use).
	///
	/// # Arguments
	///
	/// * `signal` - The signal gate for executor integration
	/// * `max_pooled_segments` - Maximum number of segments to keep in the sealed pool.
	///   - `0` = deallocate all pooled segments
	///   - `N` = keep N segments pooled
	///   - `usize::MAX` = disable deallocation (unbounded pool)
	///
	/// # Safety
	///
	/// Same safety requirements as `new_unsafe()`.
	pub unsafe fn new_unsafe_with_gate_and_config(
		signal: SignalGate,
		max_pooled_segments: usize,
	) -> Self {
		let mut v = Vec::with_capacity(Self::NUM_SEGS);
		for _ in 0..Self::NUM_SEGS {
			v.push(AtomicPtr::new(ptr::null_mut()));
		}

		Self {
			segs: v.into_boxed_slice(),
			producer: CachePadded::new(ProducerState::new()),
			consumer: CachePadded::new(ConsumerState::new()),
			signal: CachePadded::new(Some(signal)),
			max_pooled_segments,
		}
	}

	/// Returns the maximum number of items this queue can hold.
	///
	/// Formula: `(SEG_SIZE × NUM_SEGS) - 1` (one-empty-slot distinguishes full/empty)
	///
	/// # Example
	///
	/// ```ignore
	/// type Q = SegSpsc<u64, 8, 10>;  // 256 × 1024 = 262,144
	/// assert_eq!(Q::capacity(), 262_143);
	/// ```
	#[inline(always)]
	pub const fn capacity() -> usize {
		(Self::SEG_SIZE * Self::NUM_SEGS) - 1
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Signal Integration API
	// ──────────────────────────────────────────────────────────────────────────────

	/// Schedules this queue for execution via the associated `SignalGate`.
	///
	/// This is the primary mechanism for notifying the executor that this queue has work
	/// available. When called, it attempts to set the queue's bit in the signal word and
	/// potentially mark the signal group as active in the waker's summary bitmap.
	///
	/// # Signal Protocol
	///
	/// The signal protocol follows a state machine with three states: IDLE, SCHEDULED, and EXECUTING.
	/// `schedule()` attempts to transition from IDLE to SCHEDULED. If the queue is already
	/// SCHEDULED or EXECUTING, the call is a no-op (returns false internally).
	///
	/// # Integration with SignalWaker
	///
	/// When a queue transitions from IDLE to SCHEDULED via this method:
	/// 1. The bit is set in the signal word (64-bit bitmap)
	/// 2. If the signal word was previously empty, the corresponding bit is set in the summary
	/// 3. The waker may wake a sleeping executor thread if permits are available
	///
	/// # Usage
	///
	/// Typically called by producers after enqueuing items to notify consumers:
	///
	/// ```ignore
	/// queue.try_push(item)?;
	/// queue.schedule();  // Wake executor to process the item
	/// ```
	///
	/// # Performance
	///
	/// - No-op if no signal gate configured (0 ns)
	/// - ~2-5 ns if already scheduled/executing
	/// - ~10-20 ns for successful schedule (includes atomic ops + potential waker notification)
	#[inline(always)]
	pub fn schedule(&self) {
		if let Some(s) = &self.signal.as_ref() {
			let _ = s.schedule();
		}
	}

	/// Marks this queue as EXECUTING, preventing redundant scheduling.
	///
	/// This is an internal method used by the executor when it begins processing a queue.
	/// The transition from SCHEDULED to EXECUTING prevents concurrent schedule() calls
	/// from redundantly setting the signal bit.
	///
	/// # Executor Protocol
	///
	/// 1. Executor selects a set bit from the signal word
	/// 2. Calls `mark()` to transition SCHEDULED → EXECUTING
	/// 3. Processes items from the queue
	/// 4. Calls `unmark()` or `unmark_and_schedule()` depending on whether more work remains
	///
	/// # Safety
	///
	/// Must be called by the executor thread that owns this queue's timeslice.
	/// The signal protocol assumes single-threaded consumption.
	#[inline(always)]
	pub(crate) fn mark(&self) {
		if let Some(s) = &self.signal.as_ref() {
			let _ = s.mark();
		}
	}

	/// Marks this queue as IDLE, completing the current execution cycle.
	///
	/// Used by the executor after processing a batch of items. If items were added
	/// during execution (indicated by the SCHEDULED flag being set), the queue will
	/// automatically reschedule itself.
	///
	/// # Executor Protocol
	///
	/// ```ignore
	/// queue.mark();  // SCHEDULED → EXECUTING
	/// while let Ok(item) = queue.try_pop() {
	///     process(item);
	/// }
	/// queue.unmark();  // EXECUTING → IDLE (or SCHEDULED if new items arrived)
	/// ```
	///
	/// # Automatic Rescheduling
	///
	/// If producers called `schedule()` while this queue was EXECUTING, the SCHEDULED
	/// flag will be set. When `unmark()` is called, it detects this and re-sets the
	/// signal bit, ensuring the queue remains visible to the executor.
	#[inline(always)]
	pub(crate) fn unmark(&self) {
		if let Some(s) = &self.signal.as_ref() {
			let _ = s.unmark();
		}
	}

	/// Atomically marks the queue as IDLE and schedules it for re-execution.
	///
	/// This is an optimized version of calling `unmark()` followed by `schedule()`.
	/// Used when the executor knows there are more items to process but wants to
	/// yield the timeslice to ensure fairness across queues.
	///
	/// # Use Case
	///
	/// When processing large batches, the executor may want to:
	/// 1. Process N items
	/// 2. Yield to other queues
	/// 3. Ensure this queue gets rescheduled
	///
	/// ```ignore
	/// for _ in 0..BATCH_SIZE {
	///     if let Ok(item) = queue.try_pop() {
	///         process(item);
	///     } else {
	///         break;
	///     }
	/// }
	/// if queue.len() > 0 {
	///     queue.unmark_and_schedule();  // More work, reschedule
	/// } else {
	///     queue.unmark();  // Done
	/// }
	/// ```
	#[inline(always)]
	pub(crate) fn unmark_and_schedule(&self) {
		if let Some(s) = &self.signal.as_ref() {
			let _ = s.unmark_and_schedule();
		}
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Inspection API
	// ──────────────────────────────────────────────────────────────────────────────

	/// Returns the number of items currently in the queue.
	///
	/// This is calculated as `head - tail`, where both positions are monotonically
	/// increasing u64 counters. The close bit is masked out before subtraction.
	///
	/// # Close Bit Handling
	///
	/// The head position may have the close bit (bit 63) set. This method masks it out:
	/// ```ignore
	/// let h_pos = h & RIGHT_MASK;  // Clear bit 63
	/// let length = h_pos.saturating_sub(t);  // Prevent underflow
	/// ```
	///
	/// # Consistency
	///
	/// Uses `Ordering::Acquire` for both loads to ensure visibility of preceding writes.
	/// However, the value may be stale by the time the caller uses it (concurrent operations).
	///
	/// # Performance
	///
	/// ~2-5 ns (two atomic loads + subtraction)
	///
	/// # Example
	///
	/// ```ignore
	/// queue.try_push(1)?;
	/// queue.try_push(2)?;
	/// assert_eq!(queue.len(), 2);
	/// ```
	#[inline(always)]
	pub fn len(&self) -> usize {
		let h = self.producer.head.load(Ordering::Acquire);
		let t = self.consumer.tail.load(Ordering::Acquire);
		// Mask out the close bit when calculating length
		let h_pos = h & Self::RIGHT_MASK;
		h_pos.saturating_sub(t) as usize
	}

	/// Returns `true` if the queue contains no items.
	///
	/// Equivalent to `self.len() == 0`, but more readable.
	///
	/// # Note
	///
	/// This check is not atomic with subsequent operations. A producer may enqueue
	/// items immediately after this returns `true`.
	///
	/// # Example
	///
	/// ```ignore
	/// assert!(queue.is_empty());
	/// queue.try_push(1)?;
	/// assert!(!queue.is_empty());
	/// ```
	#[inline(always)]
	pub fn is_empty(&self) -> bool {
		self.len() == 0
	}

	/// Returns `true` if the queue is at maximum capacity.
	///
	/// When the queue is full, `try_push()` and `try_push_n()` will return `PushError::Full`.
	///
	/// # Capacity
	///
	/// The capacity is `(segment_size * num_segments) - 1`:
	/// - For `SegSpsc<T, 10, 0>`: `1024 - 1 = 1023` items
	/// - For `SegSpsc<T, 13, 2>`: `(8192 * 4) - 1 = 32767` items
	///
	/// # Note
	///
	/// Like `is_empty()`, this is not atomic with subsequent operations.
	///
	/// # Example
	///
	/// ```ignore
	/// for i in 0..queue.capacity() {
	///     queue.try_push(i).unwrap();
	/// }
	/// assert!(queue.is_full());
	/// assert!(queue.try_push(999).is_err());  // Returns PushError::Full
	/// ```
	#[inline(always)]
	pub fn is_full(&self) -> bool {
		self.len() == Self::capacity()
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Close API
	// ──────────────────────────────────────────────────────────────────────────────

	/// Checks if the queue has been closed.
	///
	/// The close state is encoded in bit 63 of the head position, allowing lock-free
	/// detection without a separate atomic flag.
	///
	/// # Close Bit Encoding
	///
	/// ```text
	/// Head Position (64 bits):
	/// ┌─────────┬────────────────────────────────────────────────┐
	/// │ Bit 63  │ Bits 62..0                                     │
	/// │ (Close) │ (Monotonic Position)                           │
	/// └─────────┴────────────────────────────────────────────────┘
	/// ```
	///
	/// # Semantics
	///
	/// Once closed:
	/// - All `try_push()` and `try_push_n()` calls return `PushError::Closed`
	/// - Consumers can drain remaining items
	/// - `try_pop_n()` returns `PopError::Closed` when queue is empty
	///
	/// # Memory Ordering
	///
	/// Uses `Ordering::Relaxed` because close state doesn't need to synchronize with
	/// data operations (producers/consumers already use Acquire/Release for data visibility).
	///
	/// # Performance
	///
	/// ~1-2 ns (single atomic load + bit test)
	///
	/// # Example
	///
	/// ```ignore
	/// queue.try_push(1)?;
	/// queue.close();
	/// assert!(queue.is_closed());
	/// assert!(matches!(queue.try_push(2), Err(PushError::Closed(_))));
	/// assert_eq!(queue.try_pop(), Some(1));  // Can still drain
	/// ```
	#[inline(always)]
	pub fn is_closed(&self) -> bool {
		self.producer.head.load(Ordering::Relaxed) & Self::CLOSED_CHANNEL_MASK != 0
	}

	/// Closes the queue by setting the close bit in the head position.
	///
	/// This is a one-way operation - once closed, the queue cannot be reopened.
	/// The close bit is set atomically using `fetch_or`, making it safe to call
	/// from any thread.
	///
	/// # Behavior After Close
	///
	/// - **Producers**: All enqueue operations fail with `PushError::Closed`
	/// - **Consumers**: Can drain remaining items, then receive `PopError::Closed`
	/// - **Signal**: The queue can still be scheduled/unscheduled
	///
	/// # Use Cases
	///
	/// 1. **Graceful Shutdown**: Signal that no more items will be produced
	/// 2. **Bounded Work**: Close after enqueueing a finite batch
	/// 3. **Error Propagation**: Close to signal upstream failure
	///
	/// # Memory Ordering
	///
	/// Uses `Ordering::Relaxed` - the close bit is independent of data visibility.
	/// Producers and consumers use Acquire/Release for proper data synchronization.
	///
	/// # Thread Safety
	///
	/// Safe to call from any thread, including concurrent calls (idempotent).
	///
	/// # Performance
	///
	/// ~2-5 ns (atomic fetch_or operation)
	///
	/// # Example
	///
	/// ```ignore
	/// // Producer thread
	/// for item in work_items {
	///     queue.try_push(item)?;
	/// }
	/// queue.close();  // Signal completion
	///
	/// // Consumer thread
	/// loop {
	///     match queue.try_pop() {
	///         Some(item) => process(item),
	///         None if queue.is_closed() => break,  // Done
	///         None => wait_for_work(),
	///     }
	/// }
	/// ```
	#[inline(always)]
	pub unsafe fn close(&self) {
		self.producer
				.head
				.fetch_or(Self::CLOSED_CHANNEL_MASK, Ordering::Relaxed);
		self.schedule();
	}

	/// Increments the head position while preserving the close bit.
	///
	/// This internal helper is designed for future CAS-based push operations where
	/// we need to atomically advance the head position without clearing the close bit.
	///
	/// # Implementation Note
	///
	/// Currently unused but kept for potential optimizations. The current `try_push_n`
	/// uses a simpler non-atomic increment since the producer is single-threaded.
	///
	/// # Preconditions
	///
	/// - `head_pos` must have the close bit clear (asserted in debug builds)
	/// - Used only in single-producer context
	///
	/// # Algorithm
	///
	/// ```text
	/// Given head_pos = POSITION (close bit clear)
	/// 1. Increment: new_pos = POSITION + 1
	/// 2. Check if within capacity
	/// 3. If yes: return new_pos
	/// 4. If no: wrap around (shouldn't happen with monotonic design)
	/// ```
	///
	/// # Future Use
	///
	/// If we add multi-producer support or optimistic CAS loops:
	/// ```ignore
	/// loop {
	///     let h = self.producer.head.load(Ordering::Relaxed);
	///     if h & CLOSED_CHANNEL_MASK != 0 {
	///         return Err(PushError::Closed);
	///     }
	///     let next_h = self.next_head_pos(h & RIGHT_MASK);
	///     if self.producer.head.compare_exchange_weak(h, next_h, ...).is_ok() {
	///         break;
	///     }
	/// }
	/// ```
	#[allow(dead_code)]
	#[inline(always)]
	fn next_head_pos(&self, head_pos: u64) -> u64 {
		debug_assert_eq!(
			head_pos & Self::CLOSED_CHANNEL_MASK,
			0,
			"close bit should be clear"
		);

		let new_head_pos = head_pos + 1;
		let new_index = new_head_pos & Self::RIGHT_MASK;

		// Check if we need to wrap the index
		if new_index < Self::capacity() as u64 {
			new_head_pos
		} else {
			// Wrap: increment sequence count, reset index
			let sequence_increment = Self::RIGHT_MASK + 1;
			let sequence_count = head_pos & !Self::RIGHT_MASK;
			sequence_count.wrapping_add(sequence_increment)
		}
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Producer API
	// ──────────────────────────────────────────────────────────────────────────────

	/// Attempts to enqueue a single item.
	///
	/// This is a convenience wrapper around `try_push_n()` for single-item operations.
	///
	/// # Returns
	///
	/// - `Ok(())`: Item successfully enqueued
	/// - `Err(PushError::Full(item))`: Queue is at capacity, item returned
	/// - `Err(PushError::Closed(item))`: Queue has been closed, item returned
	///
	/// # Close Semantics
	///
	/// If the queue is closed (bit 63 of head is set), this returns `Err(PushError::Closed)`.
	/// The close check happens atomically before any capacity calculations.
	///
	/// # Signal Integration
	///
	/// After successful enqueue, automatically calls `schedule()` if a signal gate is configured.
	/// This wakes the executor to process the newly available item.
	///
	/// # Performance
	///
	/// ~5-15 ns for successful push (cache hit, no segment boundary crossing)
	/// ~20-50 ns for segment allocation (first push to a new segment)
	///
	/// # Example
	///
	/// ```ignore
	/// match queue.try_push(42) {
	///     Ok(()) => println!("Enqueued successfully"),
	///     Err(PushError::Full(item)) => println!("Queue full, item={}", item),
	///     Err(PushError::Closed(item)) => println!("Queue closed, item={}", item),
	/// }
	/// ```
	///
	/// # Thread Safety
	///
	/// Safe to call from the single producer thread. NOT safe for concurrent producers
	/// (SPSC guarantee must be maintained by the caller).
	#[inline(always)]
	pub fn try_push(&self, item: T) -> Result<(), PushError<T>> {
		self.try_push_n(core::slice::from_ref(&item))
				.map(|_| ())
				.map_err(|err| match err {
					PushError::Full(()) => PushError::Full(item),
					PushError::Closed(()) => PushError::Closed(item),
				})
	}

	/// Attempts to enqueue multiple items from a slice.
	///
	/// Copies items from `src` into the queue, potentially spanning multiple segments.
	/// Returns the number of items successfully copied (may be partial if queue fills).
	///
	/// # Algorithm
	///
	/// 1. **Close Check**: Return `Err(Closed)` if bit 63 is set in head
	/// 2. **Capacity Check**: Calculate free space using cached tail (refresh if needed)
	/// 3. **Bulk Copy**: Copy min(src.len(), free) items across segment boundaries
	/// 4. **Head Update**: Atomically publish new head position with Release ordering
	/// 5. **Signal**: Call `schedule()` to wake executor
	///
	/// # Tail Caching
	///
	/// The producer maintains a cached copy of the consumer's tail position:
	/// - **Fast path**: If cached tail shows available space, skip atomic load
	/// - **Slow path**: If cached tail shows full, refresh from consumer's atomic tail
	///
	/// This optimization eliminates cache line bouncing in the common case where the
	/// queue has available space.
	///
	/// # Segment Boundaries
	///
	/// Items may span multiple segments. For each segment:
	/// ```ignore
	/// seg_idx = (position >> P) & DIR_MASK;  // Which segment in directory
	/// offset = position & SEG_MASK;          // Offset within segment
	/// ```
	///
	/// When crossing a boundary (offset wraps to 0), the next segment is lazily allocated
	/// via `ensure_segment_for()`.
	///
	/// # Memory Ordering
	///
	/// - Tail cache: `Acquire` (when refreshing from consumer)
	/// - Head update: `Release` (publishes data + position to consumer)
	///
	/// # Returns
	///
	/// - `Ok(n)`: `n` items successfully enqueued (0 ≤ n ≤ src.len())
	/// - `Err(PushError::Full(()))`: Queue full, no items enqueued
	/// - `Err(PushError::Closed(()))`: Queue closed, no items enqueued
	///
	/// # Partial Success
	///
	/// If `src.len() > free`, only `free` items are copied and `Ok(free)` is returned.
	/// The caller must check the return value to handle partial writes:
	///
	/// ```ignore
	/// let items = vec![1, 2, 3, 4, 5];
	/// match queue.try_push_n(&items) {
	///     Ok(n) if n == items.len() => println!("All items enqueued"),
	///     Ok(n) => println!("Partial: enqueued {}/{}", n, items.len()),
	///     Err(PushError::Full(())) => println!("Queue full"),
	///     Err(PushError::Closed(())) => println!("Queue closed"),
	/// }
	/// ```
	///
	/// # Performance
	///
	/// - **Single segment**: ~10-20 ns (one memcpy, no allocation)
	/// - **Multiple segments**: ~30-50 ns per segment boundary (allocation + memcpy)
	/// - **Throughput**: ~500M-1B ops/sec on modern x86_64 (depending on item size)
	///
	/// # Safety
	///
	/// Uses `ptr::copy_nonoverlapping` internally. Safety invariants:
	/// - Source slice is valid for reads
	/// - Destination segment is allocated and valid for writes
	/// - No overlap between source and destination (guaranteed by design)
	/// - Size calculation accounts for `sizeof::<T>()`
	pub fn try_push_n(&self, src: &[T]) -> Result<usize, PushError<()>> {
		let h = self.producer.head.load(Ordering::Relaxed);

		// Check if queue is closed
		if h & Self::CLOSED_CHANNEL_MASK != 0 {
			return Err(PushError::Closed(()));
		}

		// Use cached tail value, refresh if needed
		let mut t = unsafe { *self.producer.tail_cache.get() };
		let mut free = Self::capacity() - (h - t) as usize;

		if free == 0 {
			// Refresh tail cache from consumer
			t = self.consumer.tail.load(Ordering::Acquire);
			unsafe {
				*self.producer.tail_cache.get() = t;
			}
			free = Self::capacity() - (h - t) as usize;
			if free == 0 {
				return Err(PushError::Full(()));
			}
		}

		let mut n = src.len().min(free);
		let mut i = h;
		let mut copied = 0usize;

		while n > 0 {
			let seg_idx = ((i >> P) as usize) & Self::DIR_MASK;
			let off = (i as usize) & Self::SEG_MASK;
			let base = self.ensure_segment_for(seg_idx);
			let can = (Self::SEG_SIZE - off).min(n);
			unsafe {
				let dst_ptr = base.add(off) as *mut T as *mut core::ffi::c_void;
				let src_ptr = src.as_ptr().add(copied) as *const core::ffi::c_void;
				ptr::copy_nonoverlapping(
					src_ptr as *const u8,
					dst_ptr as *mut u8,
					can * core::mem::size_of::<T>(),
				);
			}

			i += can as u64;
			copied += can;
			n -= can;

			if ((i as usize) & Self::SEG_MASK) == 0 {
				let _ = self.segs[((i >> P) as usize) & Self::DIR_MASK].load(Ordering::Relaxed);
			}
		}

		self.producer.head.store(i, Ordering::Release);

		if let Some(s) = &self.signal.as_ref() {
			let _ = s.schedule();
		}
		Ok(copied)
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Consumer API (copying)
	// ──────────────────────────────────────────────────────────────────────────────

	/// Attempts to dequeue a single item by copying.
	///
	/// This is a convenience wrapper around `try_pop_n()` for single-item operations.
	/// The item is copied out of the queue into the returned `Option`.
	///
	/// # Returns
	///
	/// - `Some(item)`: Item successfully dequeued
	/// - `None`: Queue is empty or closed
	///
	/// # Distinguishing Empty vs Closed
	///
	/// This method returns `None` for both empty and closed states. Use `try_pop_n()` or
	/// check `is_closed()` separately if you need to distinguish:
	///
	/// ```ignore
	/// match queue.try_pop() {
	///     Some(item) => process(item),
	///     None if queue.is_closed() => break,  // Closed
	///     None => wait_for_items(),            // Empty
	/// }
	/// ```
	///
	/// # Head Caching
	///
	/// The consumer maintains a cached copy of the producer's head position to avoid
	/// unnecessary cache line bouncing. Only refreshes when the cached value shows empty.
	///
	/// # Performance
	///
	/// ~5-15 ns for successful pop (cache hit, no segment boundary crossing)
	///
	/// # Safety
	///
	/// Initializes return value with `mem::zeroed()`. Safe because:
	/// - If pop succeeds (returns `Some`), the zeroed value is overwritten
	/// - If pop fails (returns `None`), the zeroed value is discarded
	///
	/// # Example
	///
	/// ```ignore
	/// queue.try_push(42)?;
	/// assert_eq!(queue.try_pop(), Some(42));
	/// assert_eq!(queue.try_pop(), None);  // Empty
	/// ```
	#[inline(always)]
	pub fn try_pop(&self) -> Option<T> {
		let mut tmp: T = unsafe { core::mem::zeroed() };
		match self.try_pop_n(core::slice::from_mut(&mut tmp)) {
			Ok(1) => Some(tmp),
			_ => None,
		}
	}

	/// Attempts to dequeue multiple items by copying into the destination slice.
	///
	/// Copies items from the queue into `dst`, potentially spanning multiple segments.
	/// Returns the number of items successfully copied (may be partial if queue empties).
	///
	/// # Algorithm
	///
	/// 1. **Availability Check**: Calculate available items using cached head (refresh if needed)
	/// 2. **Close Check**: Return `Err(PopError::Closed)` if empty and bit 63 is set
	/// 3. **Bulk Copy**: Copy min(dst.len(), avail) items across segment boundaries
	/// 4. **Tail Update**: Atomically publish new tail position with Release ordering
	/// 5. **Segment Sealing**: Check if segments can be sealed and returned to pool
	///
	/// # Head Caching
	///
	/// The consumer maintains a cached copy of the producer's head position:
	/// - **Fast path**: If cached head shows available items, skip atomic load
	/// - **Slow path**: If cached head shows empty, refresh from producer's atomic head
	///
	/// This optimization eliminates cache line bouncing in the common case where the
	/// queue has available items.
	///
	/// # Close Bit Handling
	///
	/// The head position may have bit 63 set to indicate closure:
	/// ```ignore
	/// let h_pos = h & RIGHT_MASK;  // Mask out close bit
	/// let avail = h_pos.saturating_sub(t);
	/// if avail == 0 && (h & CLOSED_CHANNEL_MASK != 0) {
	///     return Err(PopError::Closed);
	/// }
	/// ```
	///
	/// # Segment Boundaries
	///
	/// Items may span multiple segments. For each segment:
	/// ```ignore
	/// seg_idx = (position >> P) & DIR_MASK;  // Which segment in directory
	/// offset = position & SEG_MASK;          // Offset within segment
	/// ```
	///
	/// After consumption, segments in the range [sealable_lo, sealable_hi) are marked
	/// as sealed and available for reuse via the pooling mechanism.
	///
	/// # Memory Ordering
	///
	/// - Head cache: `Acquire` (when refreshing from producer)
	/// - Segment load: `Acquire` (ensures data visibility)
	/// - Tail update: `Release` (publishes consumed position to producer)
	///
	/// # Returns
	///
	/// - `Ok(n)`: `n` items successfully dequeued (0 ≤ n ≤ dst.len())
	/// - `Err(PopError::Empty)`: Queue is empty
	/// - `Err(PopError::Closed)`: Queue is closed and empty
	///
	/// # Partial Success
	///
	/// If `dst.len() > avail`, only `avail` items are copied and `Ok(avail)` is returned:
	///
	/// ```ignore
	/// let mut buffer = vec![0; 100];
	/// match queue.try_pop_n(&mut buffer) {
	///     Ok(n) => println!("Dequeued {} items", n),
	///     Err(PopError::Empty) => println!("Queue empty"),
	///     Err(PopError::Closed) => println!("Queue closed"),
	/// }
	/// ```
	///
	/// # Performance
	///
	/// - **Single segment**: ~10-20 ns (one memcpy)
	/// - **Multiple segments**: ~20-30 ns per segment boundary (memcpy + sealing check)
	/// - **Throughput**: ~500M-1B ops/sec on modern x86_64 (depending on item size)
	///
	/// # Safety
	///
	/// Uses `ptr::copy_nonoverlapping` internally. Safety invariants:
	/// - Source segment is allocated and contains initialized items
	/// - Destination slice is valid for writes
	/// - No overlap between source and destination (guaranteed by design)
	/// - Items are `Copy`, so no double-drop concerns
	pub fn try_pop_n(&self, dst: &mut [T]) -> Result<usize, PopError> {
		let t = self.consumer.tail.load(Ordering::Relaxed);

		// Use cached head value, refresh if needed
		let mut h = unsafe { *self.consumer.head_cache.get() };
		// Mask out close bit to get actual position
		let h_pos = h & Self::RIGHT_MASK;
		let mut avail = h_pos.saturating_sub(t) as usize;

		if avail == 0 {
			// Refresh head cache from producer
			h = self.producer.head.load(Ordering::Acquire);
			unsafe {
				*self.consumer.head_cache.get() = h;
			}
			let h_pos = h & Self::RIGHT_MASK;
			avail = h_pos.saturating_sub(t) as usize;
			if avail == 0 {
				// Check if closed
				if h & Self::CLOSED_CHANNEL_MASK != 0 {
					return Err(PopError::Closed);
				}
				return Err(PopError::Empty);
			}
		}

		let mut n = dst.len().min(avail);

		let mut i = t;
		let mut copied = 0usize;

		while n > 0 {
			let seg_idx = ((i >> P) as usize) & Self::DIR_MASK;
			let off = (i as usize) & Self::SEG_MASK;

			let base = self.segs[seg_idx].load(Ordering::Acquire);
			debug_assert!(!base.is_null());

			let can = (Self::SEG_SIZE - off).min(n);
			unsafe {
				let src_ptr = (base.add(off) as *const MaybeUninit<T>) as *const T;
				let dst_ptr = dst.as_mut_ptr().add(copied);
				ptr::copy_nonoverlapping(src_ptr, dst_ptr, can);
			}

			i += can as u64;
			copied += can;
			n -= can;

			// Seal segment if we just finished it
			if ((i as usize) & Self::SEG_MASK) == 0 && i > 0 {
				let s_done = (i >> P).wrapping_sub(1);
				self.seal_after(s_done);
			}
		}

		self.consumer.tail.store(i, Ordering::Release);
		Ok(copied)
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Consumer API (ZERO-COPY, process-in-place)
	// ──────────────────────────────────────────────────────────────────────────────

	/// Zero-copy consumption: processes items in-place via callback with read-only slices.
	///
	/// This method provides direct read access to items in the queue without copying.
	/// The callback `f` is invoked with contiguous slices of items (up to segment boundaries),
	/// and must return the number of items it actually consumed.
	///
	/// # Zero-Copy Design
	///
	/// Unlike `try_pop_n()` which copies items into a destination buffer, this method:
	/// - Exposes items directly in their segment storage (zero memory copies)
	/// - Allows processing without intermediate allocation
	/// - Ideal for serialization, hashing, or forwarding to I/O
	///
	/// # Algorithm
	///
	/// 1. **Availability Check**: Calculate available items using cached head
	/// 2. **Segment Iteration**: For each segment in [tail, tail + max):
	///    - Calculate contiguous slice within segment boundary
	///    - Invoke callback with read-only slice
	///    - Advance tail by the number of items callback consumed
	///    - Seal segment if boundary crossed
	/// 3. **Early Exit**: Stop if callback returns 0 (processed fewer than offered)
	///
	/// # Callback Contract
	///
	/// ```ignore
	/// fn callback(slice: &[T]) -> usize
	/// ```
	///
	/// - **Input**: Read-only slice of contiguous items (may be less than requested due to segment boundaries)
	/// - **Output**: Number of items consumed (0 ≤ consumed ≤ slice.len())
	/// - **Lifetime**: Slice MUST NOT be held beyond the callback scope (undefined behavior)
	/// - **Early Exit**: Return < slice.len() to stop iteration
	///
	/// # Segment Boundaries
	///
	/// Items are presented in segment-sized chunks. If you request 10,000 items but segment
	/// size is 1024, the callback will be invoked multiple times:
	/// - 1st call: slice of up to 1024 items (or fewer if at segment boundary)
	/// - 2nd call: next slice of up to 1024 items
	/// - ... (continues until `max` items processed or callback returns 0)
	///
	/// # Close Semantics
	///
	/// This method does NOT check the close bit or return an error. It simply processes
	/// whatever items are available. Use `is_closed()` separately if needed:
	///
	/// ```ignore
	/// loop {
	///     let consumed = queue.consume_in_place(1024, |slice| {
	///         serialize(slice);
	///         slice.len()  // Consumed all
	///     });
	///
	///     if consumed == 0 {
	///         if queue.is_closed() {
	///             break;  // Done
	///         }
	///         wait_for_items();
	///     }
	/// }
	/// ```
	///
	/// # Parameters
	///
	/// - `max`: Maximum number of items to consume (may consume fewer if queue empties or callback exits early)
	/// - `f`: Callback invoked with read-only slices, returning number of items consumed
	///
	/// # Returns
	///
	/// Total number of items consumed across all callback invocations.
	///
	/// # Examples
	///
	/// **Example 1: Serialize to network without copying**
	/// ```ignore
	/// let bytes_sent = queue.consume_in_place(batch_size, |items| {
	///     match socket.write_all(bytemuck::cast_slice(items)) {
	///         Ok(()) => items.len(),     // Consumed all
	///         Err(_) => 0,               // Error, stop
	///     }
	/// });
	/// ```
	///
	/// **Example 2: Compute hash without allocation**
	/// ```ignore
	/// let mut hasher = Blake3::new();
	/// queue.consume_in_place(usize::MAX, |items| {
	///     hasher.update(bytemuck::cast_slice(items));
	///     items.len()  // Consumed all
	/// });
	/// let hash = hasher.finalize();
	/// ```
	///
	/// **Example 3: Conditional processing with early exit**
	/// ```ignore
	/// let mut count = 0;
	/// queue.consume_in_place(1000, |items| {
	///     for (i, item) in items.iter().enumerate() {
	///         if !should_process(item) {
	///             return i;  // Stop here
	///         }
	///         process(item);
	///         count += 1;
	///     }
	///     items.len()  // Consumed all
	/// });
	/// ```
	///
	/// # Performance
	///
	/// - **Throughput**: 2-5 GB/s for simple processing (memory bandwidth limited)
	/// - **Latency**: ~5-10 ns per callback invocation overhead
	/// - **Advantage over copying**: Saves ~50-200 ns per 1KB of data (avoids memcpy)
	///
	/// # Safety
	///
	/// This method is safe because:
	/// - Items are `Copy`, so no drop concerns
	/// - Producer can't modify consumed items (monotonic head/tail invariant)
	/// - Consumer is single-threaded (SPSC guarantee)
	/// - Slices are bounded to initialized region [tail, head)
	///
	/// **CRITICAL**: The callback MUST NOT store the slice reference beyond its scope.
	/// The slice becomes invalid after the tail is updated.
	pub fn consume_in_place<F>(&self, max: usize, mut f: F) -> usize
	where
			F: FnMut(&[T]) -> usize,
	{
		if max == 0 {
			return 0;
		}

		let mut t = self.consumer.tail.load(Ordering::Relaxed);

		// Use cached head value, refresh if needed
		let mut h = unsafe { *self.consumer.head_cache.get() };
		let h_pos = h & Self::RIGHT_MASK;
		let mut avail = h_pos.saturating_sub(t) as usize;

		if avail == 0 {
			// Refresh head cache from producer
			h = self.producer.head.load(Ordering::Acquire);
			unsafe {
				*self.consumer.head_cache.get() = h;
			}
			let h_pos = h & Self::RIGHT_MASK;
			avail = h_pos.saturating_sub(t) as usize;
			if avail == 0 {
				return 0;
			}
		}

		let mut remaining = max.min(avail);
		let mut total_consumed = 0usize;

		while remaining > 0 {
			let seg_idx = ((t >> P) as usize) & Self::DIR_MASK;
			let off = (t as usize) & Self::SEG_MASK;

			// directory slot must be non-null if items exist here
			let base = self.segs[seg_idx].load(Ordering::Acquire);
			// debug_assert!(!base.is_null());

			// Clamp to remain in this segment
			let in_seg = Self::SEG_SIZE - off;
			let n = remaining.min(in_seg);

			// SAFETY: producer won't touch these already-enqueued items; we expose only the
			// initialized prefix and consumer is single-threaded for this queue.
			let slice = unsafe {
				let ptr = (base.add(off) as *const MaybeUninit<T>) as *const T;
				core::slice::from_raw_parts(ptr, n)
			};

			// let took = n;
			let took = f(slice).min(n);
			if took == 0 {
				break;
			}

			t += took as u64;
			total_consumed += took;
			remaining -= took;

			// If we ended exactly at boundary, seal the finished segment
			if ((t as usize) & Self::SEG_MASK) == 0 && t > 0 {
				let s_done = (t >> P).wrapping_sub(1);
				self.seal_after(s_done);
			}

			// If callback consumed less than offered, stop
			if took < n {
				break;
			}
		}

		self.consumer.tail.store(t, Ordering::Release);
		total_consumed
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Metrics API
	// ──────────────────────────────────────────────────────────────────────────────

	/// Returns the number of fresh segment allocations (pool misses).
	///
	/// This metric tracks how many times `ensure_segment_for()` had to allocate new
	/// memory from the system allocator because no sealed segment was available for reuse.
	///
	/// # Interpretation
	///
	/// - **High value**: Indicates working set exceeds pooled segment capacity (more allocations)
	/// - **Low value**: Indicates effective pooling (segments are being reused efficiently)
	///
	/// # Use Cases
	///
	/// - **Performance tuning**: Compare fresh vs reused to assess pool effectiveness
	/// - **Capacity planning**: High fresh count may indicate need for larger `NUM_SEGS_P2`
	/// - **Memory profiling**: Track allocation patterns over time
	///
	/// # Example
	///
	/// ```ignore
	/// queue.reset_allocation_stats();
	/// // ... run workload ...
	/// println!("Fresh allocations: {}", queue.fresh_allocations());
	/// println!("Pool reuse rate: {:.1}%", queue.pool_reuse_rate());
	/// ```
	#[inline(always)]
	pub fn fresh_allocations(&self) -> u64 {
		self.producer.fresh_allocations.load(Ordering::Relaxed)
	}

	/// Returns the number of segment pool reuses (pool hits).
	///
	/// This metric tracks how many times `ensure_segment_for()` successfully reused a
	/// sealed segment from the pool instead of allocating new memory.
	///
	/// # Interpretation
	///
	/// - **High value**: Indicates effective pooling and good cache locality
	/// - **Low value**: May indicate insufficient sealed segments or high churn rate
	///
	/// # Pool Reuse Mechanism
	///
	/// Segments in the range [sealable_lo, sealable_hi) are available for reuse.
	/// When the producer needs a new segment, it first checks this range. If available,
	/// the segment is moved (not copied) to the target slot in the directory.
	///
	/// # Example
	///
	/// ```ignore
	/// let before = queue.pool_reuses();
	/// // ... enqueue/dequeue cycle that triggers segment reuse ...
	/// let after = queue.pool_reuses();
	/// println!("Segments reused: {}", after - before);
	/// ```
	#[inline(always)]
	pub fn pool_reuses(&self) -> u64 {
		self.producer.pool_reuses.load(Ordering::Relaxed)
	}

	/// Returns the segment pool reuse rate as a percentage (0-100%).
	///
	/// Calculated as: `(pool_reuses / (fresh_allocations + pool_reuses)) * 100`
	///
	/// # Interpretation
	///
	/// - **0%**: No reuse (all allocations are fresh) - may indicate cold start or insufficient pool
	/// - **50%**: Half of allocations are reused - moderate pooling effectiveness
	/// - **90%+**: Excellent reuse - pool is working well, high cache locality
	///
	/// # Performance Impact
	///
	/// - **Fresh allocation**: ~100-500 ns (system allocator + initialization)
	/// - **Pool reuse**: ~5-10 ns (pointer swap)
	///
	/// High reuse rates directly translate to lower latency and higher throughput.
	///
	/// # Example
	///
	/// ```ignore
	/// // Warm up the pool
	/// for _ in 0..queue.capacity() {
	///     queue.try_push(0)?;
	/// }
	/// for _ in 0..queue.capacity() {
	///     queue.try_pop();
	/// }
	///
	/// queue.reset_allocation_stats();
	///
	/// // Run steady-state workload
	/// for _ in 0..1_000_000 {
	///     queue.try_push(42)?;
	///     queue.try_pop();
	/// }
	///
	/// println!("Reuse rate: {:.1}%", queue.pool_reuse_rate());
	/// // Expected: ~99% after warm-up phase
	/// ```
	pub fn pool_reuse_rate(&self) -> f64 {
		let fresh = self.fresh_allocations();
		let reused = self.pool_reuses();
		let total = fresh + reused;

		if total == 0 {
			0.0
		} else {
			(reused as f64 / total as f64) * 100.0
		}
	}

	/// Resets allocation statistics to zero.
	///
	/// Useful for:
	/// - Measuring specific workload phases (reset between phases)
	/// - Repeated benchmark runs (reset between iterations)
	/// - Ignoring cold-start allocation costs (reset after warm-up)
	///
	/// # Note
	///
	/// This resets the counters for `fresh_allocations` and `pool_reuses`, but does NOT
	/// reset `total_allocated_segments` as that tracks the current live memory footprint.
	///
	/// # Example
	///
	/// ```ignore
	/// // Warm-up phase
	/// warm_up_queue(&queue);
	///
	/// // Reset stats to measure steady-state only
	/// queue.reset_allocation_stats();
	///
	/// // Run benchmark
	/// benchmark(&queue);
	///
	/// println!("Steady-state reuse rate: {:.1}%", queue.pool_reuse_rate());
	/// ```
	pub fn reset_allocation_stats(&self) {
		self.producer.fresh_allocations.store(0, Ordering::Relaxed);
		self.producer.pool_reuses.store(0, Ordering::Relaxed);
	}

	/// Returns the current number of allocated segments (live memory footprint).
	///
	/// This tracks how many segments are currently allocated (non-null) in the directory.
	/// The count increases when fresh segments are allocated and decreases when segments
	/// are deallocated due to `max_pooled_segments` limiting.
	///
	/// # Memory Usage Calculation
	///
	/// Total queue memory ≈ `allocated_segments() × SEG_SIZE × sizeof(T)` bytes
	///
	/// # Use Cases
	///
	/// - **Memory monitoring**: Track current memory footprint
	/// - **Capacity planning**: Determine if `max_pooled_segments` is effective
	/// - **Debugging**: Verify deallocation is working as expected
	///
	/// # Example
	///
	/// ```ignore
	/// let (producer, consumer) = SegSpsc::<u64, 10, 4>::new_with_config(8);
	///
	/// // Initially no segments allocated
	/// assert_eq!(producer.allocated_segments(), 0);
	///
	/// // Fill many segments during burst
	/// for i in 0..2000 {
	///     producer.try_push(i).unwrap();
	/// }
	/// let peak = producer.allocated_segments();
	/// println!("Peak allocated segments: {}", peak);
	///
	/// // Drain - should deallocate excess segments
	/// while consumer.try_pop().is_some() {}
	///
	/// let after_drain = producer.allocated_segments();
	/// println!("After drain: {} segments", after_drain);
	/// assert!(after_drain <= 8); // Limited by max_pooled_segments
	/// ```
	pub fn allocated_segments(&self) -> u64 {
		self.producer
				.total_allocated_segments
				.load(Ordering::Relaxed)
	}

	/// Attempts to deallocate excess segments from the sealed pool.
	///
	/// This method should be called when the queue is expected to be idle or empty,
	/// such as after draining all items. It will deallocate segments from the sealed
	/// pool if the pool size exceeds `max_pooled_segments`, bringing the pool down
	/// **to** the configured target (not down to zero).
	///
	/// # Safety Requirements
	///
	/// For safety, this method only deallocates when the queue is completely empty
	/// (head == tail). This prevents race conditions with active segments due to
	/// circular directory wraparound.
	///
	/// # Returns
	///
	/// The number of segments actually deallocated.
	///
	/// # Example
	///
	/// ```ignore
	/// let (producer, consumer) = SegSpsc::<u64, 10, 4>::new_with_config(16);
	///
	/// // ... run workload that allocates many segments ...
	///
	/// // Drain the queue
	/// while consumer.try_pop().is_some() {}
	///
	/// // Deallocate excess, keeping 16 segments warm in the pool
	/// let deallocated = consumer.try_deallocate_excess();
	/// println!("Deallocated {} segments", deallocated);
	/// ```
	pub fn try_deallocate_excess(&self) -> u64 {
		if self.max_pooled_segments == usize::MAX {
			return 0; // Deallocation disabled
		}
		self.try_deallocate_pool_to(self.max_pooled_segments)
	}

	/// Deallocates sealed pool segments down to a specific target size.
	///
	/// This method allows explicit control over the pool size, overriding the
	/// configured `max_pooled_segments`. Useful for manually managing memory
	/// after specific workloads.
	///
	/// # Arguments
	///
	/// * `target_pool_size` - Target number of segments to keep in the sealed pool.
	///   Segments beyond this count will be deallocated. Use 0 to deallocate all
	///   pooled segments.
	///
	/// # Safety Requirements
	///
	/// For safety, this method only deallocates when the queue is completely empty
	/// (head == tail). This prevents race conditions with active segments due to
	/// circular directory wraparound.
	///
	/// # Returns
	///
	/// The number of segments actually deallocated.
	///
	/// # Example
	///
	/// ```ignore
	/// let (producer, consumer) = SegSpsc::<u64, 10, 4>::new_with_config(16);
	///
	/// // ... run large workload ...
	///
	/// // Drain the queue
	/// while consumer.try_pop().is_some() {}
	///
	/// // Deallocate all pooled segments to free maximum memory
	/// let deallocated = consumer.deallocate_to(0);
	/// println!("Deallocated {} segments, freed {} bytes",
	///          deallocated,
	///          deallocated * (1 << 10) * 8); // assuming P=10, T=u64
	///
	/// // Or keep a smaller warm pool
	/// consumer.deallocate_to(4); // Keep only 4 segments warm
	/// ```
	pub fn deallocate_to(&self, target_pool_size: usize) -> u64 {
		self.try_deallocate_pool_to(target_pool_size)
	}

	/// Calculates the current memory usage in bytes for allocated segments.
	///
	/// Returns the total bytes used by all currently allocated segments.
	/// This does not include the fixed overhead of the directory structure itself.
	///
	/// # Formula
	///
	/// ```text
	/// memory_bytes = allocated_segments × SEG_SIZE × sizeof(T)
	/// ```
	///
	/// # Example
	///
	/// ```ignore
	/// let (producer, consumer) = SegSpsc::<u64, 10, 4>::new_with_config(8);
	///
	/// // Fill segments
	/// for i in 0..1000 {
	///     producer.try_push(i).unwrap();
	/// }
	///
	/// let bytes = producer.allocated_memory_bytes();
	/// let mb = bytes as f64 / (1024.0 * 1024.0);
	/// println!("Queue using {:.2} MB", mb);
	/// ```
	pub fn allocated_memory_bytes(&self) -> usize {
		let segments = self.allocated_segments() as usize;
		segments * Self::SEG_SIZE * core::mem::size_of::<T>()
	}

	// ──────────────────────────────────────────────────────────────────────────────
	// Internal Implementation
	// ──────────────────────────────────────────────────────────────────────────────

	/// Marks a segment as sealed (fully consumed and available for reuse).
	///
	/// Called by the consumer after crossing a segment boundary. Updates `sealable_hi`
	/// to indicate that segment `s_done` is now part of the sealed pool.
	///
	/// If `max_pooled_segments` is configured (not `usize::MAX`), this method also checks
	/// whether the sealed pool has grown too large and deallocates the oldest segments
	/// to bring it back within the configured limit.
	///
	/// # Segment Pool Lifecycle
	///
	/// ```text
	/// Directory: [seg0, seg1, seg2, seg3, ...]
	///                   ^      ^
	///                   lo     hi
	///
	/// Sealed range: [sealable_lo, sealable_hi)
	/// - Segments in this range are available for reuse by the producer
	/// - Producer calls ensure_segment_for() which steals from [lo, hi)
	/// - Consumer calls seal_after() to add newly consumed segments
	/// ```
	///
	/// # Safe Deallocation Strategy
	///
	/// When deallocating segments, we must ensure:
	/// 1. We don't deallocate segments the producer might need soon
	/// 2. We protect the active working set (segments between tail and head)
	/// 3. We handle the case where producer is at the "heel" of consumer (queue nearly full)
	///
	/// The deallocation window is calculated as:
	/// ```text
	/// pool_size = sealable_hi - sealable_lo
	/// if pool_size > max_pooled_segments:
	///     deallocate segments at positions [sealable_lo, sealable_lo + excess)
	/// ```
	///
	/// This is safe because:
	/// - Segments in [sealable_lo, sealable_hi) are fully consumed (tail has passed them)
	/// - Producer can only allocate segments at or ahead of head
	/// - Consumer advances sealable_lo when producer steals from the pool
	///
	/// # Memory Ordering
	///
	/// Uses `Ordering::Release` to ensure the segment's data writes are visible
	/// before the producer attempts to reuse the segment.
	#[inline(always)]
	fn seal_after(&self, s_done: u64) {
		let new_hi = s_done + 1;
		self.consumer.sealable_hi.store(new_hi, Ordering::Release);

		// Consumer-side garbage collection:
		// Deallocate excess segments from the sealed pool when safe to do so.
		if self.max_pooled_segments != usize::MAX {
			self.try_deallocate_pool_to(self.max_pooled_segments);
		}
	}

	/// Internal helper to deallocate sealed pool segments down to a target size.
	///
	/// # Safety
	///
	/// Only safe to call when queue is empty (tail == head) due to circular
	/// directory wraparound races. Uses CAS to claim exclusive ownership of
	/// pool positions before deallocation.
	///
	/// # Arguments
	///
	/// * `target_pool_size` - Target number of segments to keep in the pool.
	///   Excess segments beyond this will be deallocated.
	fn try_deallocate_pool_to(&self, target_pool_size: usize) -> u64 {
		let hi = self.consumer.sealable_hi.load(Ordering::Acquire);
		let lo = self.producer.sealable_lo.load(Ordering::Acquire);
		let pool_size = hi.saturating_sub(lo);

		// Check if pool exceeds the target
		if pool_size <= target_pool_size as u64 {
			return 0; // Already at or below target
		}

		// CRITICAL SAFETY: Only deallocate when queue is empty to avoid wraparound races
		// With circular indexing, segment at position N and segment at position N+NUM_SEGS
		// both map to the same dir_idx. We can only safely deallocate when there are no
		// active segments that could collide with sealed pool positions.
		let head = self.producer.head.load(Ordering::Acquire) & Self::RIGHT_MASK;
		let tail = self.consumer.tail.load(Ordering::Acquire);

		if tail != head {
			return 0; // Queue has items - not safe to deallocate
		}

		// Queue is empty - safe to deallocate excess from sealed pool
		// Strategy: Count allocated segments, then deallocate oldest to reach target

		// Important: With circular directory, multiple pool positions map to same dir_idx.
		// Count actual allocated segments by scanning directory, not pool positions.

		// First pass: count allocated segments in directory
		let mut total_allocated = 0u64;
		for i in 0..Self::NUM_SEGS {
			let seg_ptr = self.segs[i].load(Ordering::Acquire);
			if !seg_ptr.is_null() {
				total_allocated += 1;
			}
		}

		if total_allocated <= target_pool_size as u64 {
			return 0; // Already at or below target
		}

		// Second pass: deallocate from lo upwards until we've kept target_pool_size
		let need_to_dealloc = total_allocated - target_pool_size as u64;
		let mut deallocated_count = 0u64;
		let mut current_lo = lo;

		while current_lo < hi && deallocated_count < need_to_dealloc {
			let dir_idx = (current_lo as usize) & Self::DIR_MASK;

			// Try to atomically claim this pool position using CAS
			match self.producer.sealable_lo.compare_exchange(
				current_lo,
				current_lo + 1,
				Ordering::AcqRel,
				Ordering::Acquire,
			) {
				Ok(_) => {
					// Successfully claimed - check if segment is allocated
					let seg_ptr = self.segs[dir_idx].swap(ptr::null_mut(), Ordering::AcqRel);

					if !seg_ptr.is_null() {
						// Deallocate this segment
						unsafe {
							free_segment::<T>(seg_ptr, Self::SEG_SIZE);
						}
						deallocated_count += 1;
					}

					current_lo += 1;
				}
				Err(new_lo_val) => {
					// Producer claimed this position - update and continue
					current_lo = new_lo_val;
				}
			}
		}

		// Update total allocated count
		if deallocated_count > 0 {
			self.producer
					.total_allocated_segments
					.fetch_sub(deallocated_count, Ordering::Relaxed);
		}

		deallocated_count
	}

	/// Ensures a segment is allocated and ready for the given directory index.
	///
	/// This is the core segment management routine, implementing lazy allocation
	/// with pooling. Called by the producer when crossing segment boundaries.
	///
	/// # Algorithm
	///
	/// 1. **Fast path**: Check if `segs[seg_idx]` is already non-null (return immediately)
	/// 2. **Pool check**: Load [sealable_lo, sealable_hi) range
	/// 3. **Reuse path**: If sealed segments available:
	///    - Load segment from `segs[sealable_lo]`
	///    - Move to `segs[seg_idx]`
	///    - Increment `sealable_lo` and `pool_reuses` counter
	/// 4. **Allocation path**: If no sealed segments:
	///    - Allocate new segment via system allocator
	///    - Increment `fresh_allocations` counter
	///
	/// # Segment Pooling
	///
	/// Sealed segments form a FIFO pool:
	/// ```text
	/// segs[lo] → segs[lo+1] → ... → segs[hi-1]
	///  ^                               ^
	///  next to reuse                   next to seal
	/// ```
	///
	/// This provides temporal locality: recently used segments are reused first,
	/// keeping them hot in cache.
	///
	/// # Memory Ordering
	///
	/// - Segment loads: `Acquire` (ensure data visibility)
	/// - Segment stores: `Release` (publish segment availability)
	/// - Counter updates: `Relaxed` (stats don't require synchronization)
	///
	/// # Returns
	///
	/// Non-null pointer to segment storage (`*mut MaybeUninit<T>`)
	///
	/// # Safety
	///
	/// The returned pointer:
	/// - Points to valid segment storage (1 << P items)
	/// - Is uniquely owned by `segs[seg_idx]` after this call
	/// - Remains valid until segment is sealed and reused elsewhere
	fn ensure_segment_for(&self, seg_idx: usize) -> *mut MaybeUninit<T> {
		// Already available?
		let p = self.segs[seg_idx].load(Ordering::Acquire);
		if !p.is_null() {
			return p;
		}

		// Try steal from sealed prefix
		let mut hi = self.consumer.sealable_hi.load(Ordering::Acquire);
		let mut lo = self.producer.sealable_lo.load(Ordering::Acquire);

		// Check if we should deallocate excess pooled segments
		// We do this here (producer side) because we have exclusive ownership
		// of any segment we steal, making deallocation safe.
		let should_deallocate = if self.max_pooled_segments < Self::NUM_SEGS {
			let pool_size = hi.saturating_sub(lo);
			pool_size > self.max_pooled_segments as u64
		} else {
			false
		};

		// Loop to handle CAS failures when consumer updates sealable_lo concurrently
		while lo < hi {
			let src_idx = (lo as usize) & Self::DIR_MASK;

			// Try to atomically claim this pool position using CAS FIRST
			// This prevents the consumer from deallocating while we steal
			match self.producer.sealable_lo.compare_exchange(
				lo,
				lo + 1,
				Ordering::AcqRel,
				Ordering::Acquire,
			) {
				Ok(_) => {
					// Successfully claimed - now safe to take the segment
					let q = self.segs[src_idx].swap(ptr::null_mut(), Ordering::AcqRel);

					if q.is_null() {
						// No segment available in pool - fresh allocation (cache miss)
						let newp = unsafe { alloc_segment::<T>(1usize << P) };
						self.segs[seg_idx].store(newp, Ordering::Release);
						self.producer
								.fresh_allocations
								.fetch_add(1, Ordering::Relaxed);
						self.producer
								.total_allocated_segments
								.fetch_add(1, Ordering::Relaxed);
						return newp;
					} else if should_deallocate {
						// Pool is over limit - check if we should deallocate this segment
						// or if we've reached the target and can reuse it

						// Recalculate pool size after claiming this position
						let new_hi = self.consumer.sealable_hi.load(Ordering::Acquire);
						let new_lo = self.producer.sealable_lo.load(Ordering::Acquire);
						let new_pool_size = new_hi.saturating_sub(new_lo);

						if new_pool_size <= self.max_pooled_segments as u64 {
							// Pool is now within limits - reuse this segment instead of freeing it
							// This avoids wasteful free + immediate realloc
							self.segs[seg_idx].store(q, Ordering::Release);
							self.producer.pool_reuses.fetch_add(1, Ordering::Relaxed);
							return q;
						}

						// Pool still over limit - deallocate this segment and continue
						unsafe {
							free_segment::<T>(q, Self::SEG_SIZE);
						}
						self.producer
								.total_allocated_segments
								.fetch_sub(1, Ordering::Relaxed);

						// Continue loop to deallocate more segments
						lo = new_lo;
						hi = new_hi;
					} else {
						// Reuse segment from sealed pool (cache hit)
						self.segs[seg_idx].store(q, Ordering::Release);
						self.producer.pool_reuses.fetch_add(1, Ordering::Relaxed);
						return q;
					}
				}
				Err(new_lo) => {
					// Another thread claimed this position, retry with new lo
					lo = new_lo;
					// Reload hi in case it changed too
					let new_hi = self.consumer.sealable_hi.load(Ordering::Acquire);
					if new_hi > hi {
						hi = new_hi;
					}
				}
			}
		}

		// Fresh allocation (cache miss)
		let newp = unsafe { alloc_segment::<T>(1usize << P) };
		self.segs[seg_idx].store(newp, Ordering::Release);
		self.producer
				.fresh_allocations
				.fetch_add(1, Ordering::Relaxed);
		self.producer
				.total_allocated_segments
				.fetch_add(1, Ordering::Relaxed);
		newp
	}
}

impl<T, const P: usize, const NUM_SEGS_P2: usize> Drop for Spsc<T, P, NUM_SEGS_P2> {
	fn drop(&mut self) {
		for slot in self.segs.iter() {
			let p = slot.load(Ordering::Relaxed);
			if !p.is_null() {
				unsafe { free_segment::<T>(p, 1usize << P) };
			}
		}
	}
}

unsafe fn alloc_segment<T>(seg_size: usize) -> *mut MaybeUninit<T> {
	unsafe {
		let elem = core::mem::size_of::<MaybeUninit<T>>();
		let align = core::mem::align_of::<MaybeUninit<T>>();
		let layout = Layout::from_size_align_unchecked(elem * seg_size, align);
		let p = alloc_zeroed(layout) as *mut MaybeUninit<T>;
		if p.is_null() {
			std::alloc::handle_alloc_error(layout);
		}
		p
	}
}

unsafe fn free_segment<T>(ptr_base: *mut MaybeUninit<T>, seg_size: usize) {
	unsafe {
		let elem = core::mem::size_of::<MaybeUninit<T>>();
		let align = core::mem::align_of::<MaybeUninit<T>>();
		let layout = Layout::from_size_align_unchecked(elem * seg_size, align);
		dealloc(ptr_base as *mut u8, layout);
	}
}

/* ===========================
Demo / comprehensive tests
=========================== */

#[cfg(test)]
mod tests {
	use super::*;
	const P: usize = 6; // 64 items/segment
	const NUM_SEGS_P2: usize = 8;
	type Q = Spsc<u64, P, NUM_SEGS_P2>;

	#[test]
	fn basic_push_pop() {
		let q = unsafe { Q::new_unsafe() };
		for i in 0..200u64 {
			q.try_push(i).unwrap();
		}
		for i in 0..200u64 {
			assert_eq!(q.try_pop(), Some(i));
		}
		assert!(q.try_pop().is_none());
	}

	#[test]
	fn zero_copy_consume_in_place() {
		let q = unsafe { Q::new_unsafe() };
		let n = 64 * 3 + 5; // crosses segments
		for i in 0..n as u64 {
			q.try_push(i).unwrap();
		}

		let mut seen = 0u64;
		// repeatedly consume up to 50, in-place (callback may be invoked multiple times per call)
		while seen < n as u64 {
			let mut local_seen = seen;
			let took = q.consume_in_place(50, |chunk| {
				for (k, &v) in chunk.iter().enumerate() {
					assert_eq!(v, local_seen + k as u64, "Mismatch at local offset {}", k);
				}
				local_seen += chunk.len() as u64;
				// consume whole chunk
				chunk.len()
			});
			assert!(took > 0);
			seen += took as u64;
		}
	}

	#[test]
	fn test_constants_and_capacity() {
		assert_eq!(Q::SEG_SIZE, 64);
		assert_eq!(Q::NUM_SEGS, 256);
		assert_eq!(Q::SEG_MASK, 63);
		assert_eq!(Q::DIR_MASK, 255);
		assert_eq!(Q::capacity(), 64 * 256 - 1);
	}

	#[test]
	fn test_new_queue_is_empty() {
		let q = unsafe { Q::new_unsafe() };
		assert!(q.is_empty());
		assert!(!q.is_full());
		assert_eq!(q.len(), 0);
	}

	#[test]
	fn test_single_item_operations() {
		let q = unsafe { Q::new_unsafe() };
		assert!(q.is_empty());

		// Push single item
		assert!(q.try_push(42).is_ok());
		assert_eq!(q.len(), 1);
		assert!(!q.is_empty());

		// Pop single item
		assert_eq!(q.try_pop(), Some(42));
		assert_eq!(q.len(), 0);
		assert!(q.is_empty());
	}

	#[test]
	fn test_push_pop_order() {
		let q = unsafe { Q::new_unsafe() };
		let items = [1, 2, 3, 4, 5];

		for &item in &items {
			q.try_push(item).unwrap();
		}

		for &expected in &items {
			assert_eq!(q.try_pop(), Some(expected));
		}
	}

	#[test]
	fn test_try_push_n() {
		let q = unsafe { Q::new_unsafe() };
		let items = [10, 20, 30, 40, 50];

		// Push all items at once
		let pushed = q.try_push_n(&items).unwrap();
		assert_eq!(pushed, items.len());
		assert_eq!(q.len(), items.len());

		// Verify items are in correct order
		for &expected in items.iter() {
			assert_eq!(q.try_pop(), Some(expected));
		}
	}

	#[test]
	fn test_try_pop_n() {
		let q = unsafe { Q::new_unsafe() };
		let items = [1, 2, 3, 4, 5, 6, 7, 8];

		// Push all items
		for &item in &items {
			q.try_push(item).unwrap();
		}

		// Pop fewer items than available
		let mut buffer = [0u64; 5];
		let popped = q.try_pop_n(&mut buffer).unwrap();
		assert_eq!(popped, 5);
		assert_eq!(&buffer[..5], &[1, 2, 3, 4, 5]);

		// Pop remaining items
		let mut buffer2 = [0u64; 8];
		let popped2 = q.try_pop_n(&mut buffer2).unwrap();
		assert_eq!(popped2, 3);
		assert_eq!(&buffer2[..3], &[6, 7, 8]);
	}

	#[test]
	fn test_operations_on_empty_queue() {
		let q = unsafe { Q::new_unsafe() };

		// Pop from empty queue
		assert_eq!(q.try_pop(), None);

		// Pop multiple from empty queue
		let mut buffer = [0u64; 5];
		assert!(q.try_pop_n(&mut buffer).is_err());

		// Consume from empty queue
		let consumed = q.consume_in_place(10, |_| 0);
		assert_eq!(consumed, 0);
	}

	#[test]
	fn test_operations_on_full_queue() {
		let q = unsafe { Q::new_unsafe() };

		// Fill the queue completely
		let capacity = Q::capacity();
		for i in 0..capacity as u64 {
			q.try_push(i).unwrap();
		}

		assert!(q.is_full());
		assert_eq!(q.len(), capacity);

		// Try to push to full queue (single item)
		assert!(q.try_push(999).is_err());

		// Try to push multiple items to full queue
		let items = [1000, 1001, 1002];
		assert!(q.try_push_n(&items).is_err());

		// Pop one item and verify queue is no longer full
		assert_eq!(q.try_pop(), Some(0));
		assert!(!q.is_full());
		assert_eq!(q.len(), capacity - 1);
	}

	#[test]
	fn test_consume_in_place_partial() {
		let q = unsafe { Q::new_unsafe() };
		let items = [1, 2, 3, 4, 5, 6, 7, 8];

		for &item in &items {
			q.try_push(item).unwrap();
		}

		// Consume only half of what's available
		let mut consumed_count = 0;
		let consumed = q.consume_in_place(4, |chunk| {
			consumed_count += chunk.len();
			chunk.len() // consume all items in the chunk
		});

		assert_eq!(consumed, 4);
		assert_eq!(consumed_count, 4);
		assert_eq!(q.len(), 4);

		// Verify remaining items
		for expected in 5..=8 {
			assert_eq!(q.try_pop(), Some(expected));
		}
	}

	#[test]
	fn test_consume_zero_items_in_place() {
		let q = unsafe { Q::new_unsafe() };
		q.try_push(1).unwrap();
		q.try_push(2).unwrap();

		// Ask to consume zero items
		let consumed = q.consume_in_place(0, |_| 10);
		assert_eq!(consumed, 0);
		assert_eq!(q.len(), 2);
	}

	#[test]
	fn test_segment_boundaries() {
		let q = unsafe { Q::new_unsafe() };
		let seg_size = Q::SEG_SIZE;

		// Push exactly one segment
		for i in 0..seg_size as u64 {
			q.try_push(i).unwrap();
		}

		// Push one more to cross boundary
		q.try_push(seg_size as u64).unwrap();

		// Pop all and verify order
		for i in 0..=seg_size as u64 {
			assert_eq!(q.try_pop(), Some(i));
		}

		assert!(q.is_empty());
	}

	#[test]
	fn test_multiple_segments() {
		let q = unsafe { Q::new_unsafe() };
		let seg_size = Q::SEG_SIZE;
		let num_segments = 3;
		let total_items = seg_size * num_segments;

		// Push items across multiple segments
		for i in 0..total_items as u64 {
			q.try_push(i).unwrap();
		}

		// Verify all items are in correct order
		for i in 0..total_items as u64 {
			assert_eq!(q.try_pop(), Some(i));
		}

		assert!(q.is_empty());
	}

	#[test]
	fn test_large_batch_operations() {
		let q = unsafe { Q::new_unsafe() };
		let batch_size = 200;
		let mut items = Vec::with_capacity(batch_size);

		for i in 0..batch_size {
			items.push(i as u64);
		}

		// Test large batch push
		let pushed = q.try_push_n(&items).unwrap();
		assert_eq!(pushed, batch_size);

		// Test large batch pop
		let mut buffer = vec![0u64; batch_size];
		let popped = q.try_pop_n(&mut buffer).unwrap();
		assert_eq!(popped, batch_size);
		assert_eq!(&buffer[..], &items[..]);
	}

	#[test]
	fn test_alternating_push_consume() {
		let q = unsafe { Q::new_unsafe() };

		// Push some items
		for i in 0..50 {
			q.try_push(i).unwrap();
		}

		// Consume some in-place
		let consumed = q.consume_in_place(25, |chunk| chunk.len());
		assert_eq!(consumed, 25);

		// Push more items
		for i in 50..100 {
			q.try_push(i).unwrap();
		}

		// Pop all remaining
		let mut popped_items = Vec::new();
		while let Some(item) = q.try_pop() {
			popped_items.push(item);
		}

		// Verify order: items 25-99 should remain
		for (i, expected) in (25..100).enumerate() {
			assert_eq!(popped_items[i], expected);
		}
	}

	#[test]
	fn test_queue_reuse_after_drain() {
		let q = unsafe { Q::new_unsafe() };

		// First round: fill and drain
		for i in 0..1000 {
			q.try_push(i as u64).unwrap();
		}
		for i in 0..1000 {
			assert_eq!(q.try_pop(), Some(i as u64));
		}

		assert!(q.is_empty());

		// Second round: reuse the same queue
		for i in 0..500 {
			q.try_push((i + 1000) as u64).unwrap();
		}
		for i in 0..500 {
			assert_eq!(q.try_pop(), Some((i + 1000) as u64));
		}

		assert!(q.is_empty());
	}

	#[test]
	fn test_different_types() {
		// Test with different copyable types
		#[derive(Copy, Clone, Debug, PartialEq, Eq)]
		struct TestStruct {
			a: u32,
			b: i64,
		}

		type QStruct = Spsc<TestStruct, 5, 6>;
		let q = unsafe { QStruct::new_unsafe() };

		let item1 = TestStruct { a: 1, b: -1 };
		let item2 = TestStruct { a: 2, b: -2 };

		q.try_push(item1).unwrap();
		q.try_push(item2).unwrap();

		assert_eq!(q.try_pop(), Some(item1));
		assert_eq!(q.try_pop(), Some(item2));
		assert_eq!(q.try_pop(), None);
	}

	#[test]
	fn test_small_queue_configuration() {
		// Test with a smaller queue configuration
		type SmallQ = Spsc<u32, 2, 2>; // 4 items per segment, 4 segments = 15 capacity
		let q = unsafe { SmallQ::new_unsafe() };

		assert_eq!(SmallQ::capacity(), 15);
		assert_eq!(SmallQ::SEG_SIZE, 4);
		assert_eq!(SmallQ::NUM_SEGS, 4);

		// Fill to capacity
		for i in 0..15 {
			q.try_push(i as u32).unwrap();
		}

		assert!(q.is_full());
		assert!(q.try_push(100).is_err());

		// Drain completely
		for i in 0..15 {
			assert_eq!(q.try_pop(), Some(i as u32));
		}

		assert!(q.is_empty());
	}

	#[test]
	fn test_consume_with_empty_callback() {
		let q = unsafe { Q::new_unsafe() };

		// Push some items
		for i in 0..10 {
			q.try_push(i).unwrap();
		}

		// Consume with callback that returns 0 (consumes nothing)
		let consumed = q.consume_in_place(5, |_| 0);
		assert_eq!(consumed, 0);
		assert_eq!(q.len(), 10); // No items should be consumed

		// Normal consumption should still work
		let consumed2 = q.consume_in_place(5, |chunk| chunk.len());
		assert_eq!(consumed2, 5);
		assert_eq!(q.len(), 5);
	}

	#[test]
	fn test_length_tracking() {
		let q = unsafe { Q::new_unsafe() };

		assert_eq!(q.len(), 0);

		// Push items and check length
		for i in 1..=100 {
			q.try_push(i as u64).unwrap();
			assert_eq!(q.len(), i);
		}

		// Pop items and check length
		for i in (1..=100).rev() {
			q.try_pop().unwrap();
			assert_eq!(q.len(), i - 1);
		}

		assert_eq!(q.len(), 0);
	}

	#[test]
	fn test_allocation_stats_initial() {
		let q = unsafe { Q::new_unsafe() };

		// Initially no allocations should have occurred
		assert_eq!(q.fresh_allocations(), 0);
		assert_eq!(q.pool_reuses(), 0);
		assert_eq!(q.pool_reuse_rate(), 0.0);
	}

	#[test]
	fn test_allocation_stats_fresh_allocations() {
		let q = unsafe { Q::new_unsafe() };
		let seg_size = Q::SEG_SIZE;

		// Push enough items to require multiple segment allocations
		// This should trigger fresh allocations since we have a fresh queue
		let items_to_push = seg_size * 3; // Should trigger 3 segment allocations

		for i in 0..items_to_push as u64 {
			q.try_push(i).unwrap();
		}

		// Should have fresh allocations but no pool reuses yet
		let fresh_allocs = q.fresh_allocations();
		let pool_reuses = q.pool_reuses();

		assert!(
			fresh_allocs >= 3,
			"Expected at least 3 fresh allocations, got {}",
			fresh_allocs
		);
		assert_eq!(pool_reuses, 0);
		assert_eq!(q.pool_reuse_rate(), 0.0);
	}

	#[test]
	fn test_allocation_stats_pool_reuse() {
		let q = unsafe { Q::new_unsafe() };
		let seg_size = Q::SEG_SIZE;

		// Phase 1: Fill and consume to create pool entries
		let items_to_push = seg_size * 2; // Fill 2 segments

		for i in 0..items_to_push as u64 {
			q.try_push(i).unwrap();
		}

		// Consume everything to seal segments back into pool
		for i in 0..items_to_push as u64 {
			assert_eq!(q.try_pop(), Some(i));
		}

		let fresh_after_phase1 = q.fresh_allocations();
		assert!(fresh_after_phase1 >= 2);

		// Phase 2: Push more items - should reuse from pool
		for i in 0..items_to_push as u64 {
			q.try_push(i + items_to_push as u64).unwrap();
		}

		// Now we should have both fresh allocations and pool reuses
		let fresh_allocs = q.fresh_allocations();
		let pool_reuses = q.pool_reuses();

		assert!(pool_reuses > 0, "Expected pool reuses, got {}", pool_reuses);
		assert!(
			fresh_allocs > 0,
			"Expected fresh allocations, got {}",
			fresh_allocs
		);
		assert!(
			q.pool_reuse_rate() > 0.0,
			"Expected positive reuse rate, got {}",
			q.pool_reuse_rate()
		);
	}

	#[test]
	fn test_allocation_stats_reset() {
		let q = unsafe { Q::new_unsafe() };

		// Push enough items to trigger allocations
		for i in 0..(Q::SEG_SIZE * 2) as u64 {
			q.try_push(i).unwrap();
		}

		// Verify we have allocations
		assert!(q.fresh_allocations() > 0);

		// Reset stats
		q.reset_allocation_stats();

		// Verify stats are reset
		assert_eq!(q.fresh_allocations(), 0);
		assert_eq!(q.pool_reuses(), 0);
		assert_eq!(q.pool_reuse_rate(), 0.0);
	}

	#[test]
	fn test_allocation_stats_mixed_operations() {
		let q = unsafe { Q::new_unsafe() };
		let seg_size = Q::SEG_SIZE;

		// Simulate realistic usage: push some, consume some, repeat
		let mut next_pop_value = 0u64;
		for cycle in 0..5 {
			// Push items (may trigger allocations)
			let base = (cycle * seg_size) as u64;
			for i in 0..seg_size as u64 {
				q.try_push(base + i).unwrap();
			}

			// Consume half the items (may create pool entries)
			for _ in 0..(seg_size / 2) {
				assert_eq!(q.try_pop(), Some(next_pop_value));
				next_pop_value += 1;
			}
		}

		// We should have a mix of fresh allocations and pool reuses
		let fresh = q.fresh_allocations();
		let reused = q.pool_reuses();

		assert!(fresh > 0, "Expected fresh allocations: {}", fresh);

		// After several cycles, we should start seeing pool reuses
		// (though this depends on the exact pattern of segment sealing)
		println!(
			"Fresh allocations: {}, Pool reuses: {}, Reuse rate: {:.1}%",
			fresh,
			reused,
			q.pool_reuse_rate()
		);
	}

	#[test]
	fn test_allocation_stats_with_consume_in_place() {
		let q = unsafe { Q::new_unsafe() };
		let seg_size = Q::SEG_SIZE;

		// Push items to fill multiple segments
		for i in 0..(seg_size * 3) as u64 {
			q.try_push(i).unwrap();
		}

		let fresh_after_push = q.fresh_allocations();

		// Use consume_in_place to efficiently process items
		let mut total_consumed = 0;
		while total_consumed < seg_size * 3 {
			let consumed = q.consume_in_place(64, |chunk| chunk.len());
			total_consumed += consumed;
		}

		// Consume the rest normally
		while q.try_pop().is_some() {}

		// Push more items to potentially reuse segments
		for i in 0..(seg_size * 2) as u64 {
			q.try_push(i).unwrap();
		}

		// Should have some pool reuses now
		let fresh = q.fresh_allocations();
		let reused = q.pool_reuses();

		assert!(
			fresh >= fresh_after_push,
			"Fresh allocations should not decrease"
		);
		assert!(
			reused > 0 || fresh >= 5,
			"Either pool reuses should occur or we should have many fresh allocations"
		);

		if fresh > 0 {
			println!(
				"Allocation stats with consume_in_place - Fresh: {}, Reused: {}, Reuse rate: {:.1}%",
				fresh,
				reused,
				q.pool_reuse_rate()
			);
		}
	}

	#[test]
	fn test_close_bit_encoding() {
		let q = unsafe { Q::new_unsafe() };

		// Initially queue should not be closed
		assert!(!q.is_closed());

		// Push some items
		q.try_push(1).unwrap();
		q.try_push(2).unwrap();
		q.try_push(3).unwrap();

		// Queue should still not be closed
		assert!(!q.is_closed());
		assert_eq!(q.len(), 3);

		// Close the queue
		unsafe {
			q.close();
		}

		// Queue should now be closed
		assert!(q.is_closed());

		// Length should still be correct even though queue is closed
		assert_eq!(q.len(), 3);

		// Try to push after closing should fail
		assert!(matches!(q.try_push(4), Err(PushError::Closed(_))));
		assert!(matches!(q.try_push_n(&[5, 6]), Err(PushError::Closed(()))));

		// Should still be able to pop existing items
		assert_eq!(q.try_pop(), Some(1));
		assert_eq!(q.try_pop(), Some(2));
		assert_eq!(q.try_pop(), Some(3));

		// Now queue is empty and closed, should return Closed error
		assert!(matches!(q.try_pop(), None));
	}

	#[test]
	fn test_close_with_try_pop_n() {
		let q = unsafe { Q::new_unsafe() };

		// Push items
		for i in 0..10u64 {
			q.try_push(i).unwrap();
		}

		// Close the queue
		unsafe {
			q.close();
		}

		// Should be able to drain existing items
		let mut buffer = [0u64; 5];
		assert_eq!(q.try_pop_n(&mut buffer).unwrap(), 5);
		assert_eq!(&buffer, &[0, 1, 2, 3, 4]);

		// Drain remaining
		assert_eq!(q.try_pop_n(&mut buffer).unwrap(), 5);
		assert_eq!(&buffer, &[5, 6, 7, 8, 9]);

		// Now empty and closed
		assert!(matches!(q.try_pop_n(&mut buffer), Err(PopError::Closed)));
	}

	#[test]
	fn test_close_with_consume_in_place() {
		let q = unsafe { Q::new_unsafe() };

		// Push items
		for i in 0..20u64 {
			q.try_push(i).unwrap();
		}

		// Close the queue
		unsafe {
			q.close();
		}

		// Should be able to consume existing items
		let mut consumed_values = Vec::new();
		let total = q.consume_in_place(20, |chunk| {
			consumed_values.extend_from_slice(chunk);
			chunk.len()
		});

		assert_eq!(total, 20);
		assert_eq!(consumed_values.len(), 20);
		for (i, &v) in consumed_values.iter().enumerate() {
			assert_eq!(v, i as u64);
		}

		// Queue should be empty now
		assert_eq!(q.consume_in_place(10, |_| 0), 0);
	}

	#[test]
	fn test_cache_hit_miss_behavior() {
		let q = unsafe { Q::new_unsafe() };
		let seg_size = Q::SEG_SIZE;
		let num_segments = Q::NUM_SEGS;

		println!(
			"Testing cache hit/miss behavior with segment size: {}, num segments: {}",
			seg_size, num_segments
		);

		// Phase 1: Fill the queue to force fresh allocations (cache misses)
		println!("\n=== Phase 1: Fresh Allocations (Cache Misses) ===");
		for i in 0..(seg_size * 3) as u64 {
			q.try_push(i).unwrap();
		}

		let fresh_after_phase1 = q.fresh_allocations();
		let reused_after_phase1 = q.pool_reuses();

		println!("After filling 3 segments:");
		println!("  Fresh allocations (cache misses): {}", fresh_after_phase1);
		println!("  Pool reuses (cache hits): {}", reused_after_phase1);
		println!("  Reuse rate: {:.1}%", q.pool_reuse_rate());

		assert!(
			fresh_after_phase1 >= 3,
			"Should have at least 3 fresh allocations"
		);
		assert_eq!(
			reused_after_phase1, 0,
			"Should have no pool reuses initially"
		);

		// Phase 2: Drain all items to create pool entries
		println!("\n=== Phase 2: Drain to Create Pool ===");
		let mut drained = 0;
		while let Some(_) = q.try_pop() {
			drained += 1;
		}
		println!("Drained {} items", drained);
		println!("  Fresh allocations: {}", q.fresh_allocations());
		println!("  Pool reuses: {}", q.pool_reuses());

		// Phase 3: Push items again - should reuse from pool (cache hits)
		println!("\n=== Phase 3: Pool Reuse (Cache Hits) ===");
		for i in 0..(seg_size * 2) as u64 {
			q.try_push(i + 1000).unwrap();
		}

		let fresh_after_phase3 = q.fresh_allocations();
		let reused_after_phase3 = q.pool_reuses();

		println!("After refilling 2 segments:");
		println!("  Fresh allocations (cache misses): {}", fresh_after_phase3);
		println!("  Pool reuses (cache hits): {}", reused_after_phase3);
		println!("  Reuse rate: {:.1}%", q.pool_reuse_rate());

		assert!(
			reused_after_phase3 > 0,
			"Should have pool reuses: {}",
			reused_after_phase3
		);

		// Phase 4: Demonstrate alternating pattern to show mixed behavior
		println!("\n=== Phase 4: Alternating Pattern ===");
		q.reset_allocation_stats();

		for cycle in 0..3 {
			// Fill a segment
			for i in 0..seg_size as u64 {
				q.try_push(cycle * 100 + i).unwrap();
			}

			// Drain half to create some pool entries
			for _ in 0..(seg_size / 2) as u64 {
				q.try_pop().unwrap();
			}

			println!(
				"Cycle {}: Fresh={}, Reused={:.1}%",
				cycle + 1,
				q.fresh_allocations(),
				q.pool_reuse_rate()
			);
		}

		// Final statistics
		let final_fresh = q.fresh_allocations();
		let final_reused = q.pool_reuses();
		let final_rate = q.pool_reuse_rate();

		println!("\n=== Final Statistics ===");
		println!("Total fresh allocations (cache misses): {}", final_fresh);
		println!("Total pool reuses (cache hits): {}", final_reused);
		println!("Overall reuse rate: {:.1}%", final_rate);

		// Verify we have both types of allocations
		assert!(final_fresh > 0, "Should have some fresh allocations");
		// Pool reuses may or may not occur depending on exact timing of segment sealing
		println!("Cache behavior analysis complete!");
	}

	#[test]
	fn test_bounded_pool_deallocation() {
		// Test that max_pooled_segments limits memory usage during bursts
		const TEST_P: usize = 6; // 64 items/segment
		const TEST_NUM_SEGS_P2: usize = 8; // 256 segments max
		type TestQ = Spsc<u64, TEST_P, TEST_NUM_SEGS_P2>;

		const MAX_POOLED: usize = 8;
		let (producer, consumer) = TestQ::new_with_config(MAX_POOLED);

		// Phase 1: Burst - fill many segments
		println!("\n=== Phase 1: Burst - Fill Many Segments ===");
		let burst_items = 64 * 20; // 20 segments worth
		for i in 0..burst_items {
			producer.try_push(i).unwrap();
		}
		println!("Pushed {} items across ~20 segments", burst_items);
		println!("Fresh allocations: {}", producer.fresh_allocations());

		// Phase 2: Consumer drains everything
		println!("\n=== Phase 2: Consumer Drains ===");
		let mut drained = 0;
		while let Some(_) = consumer.try_pop() {
			drained += 1;
		}
		println!("Drained {} items", drained);

		// Now the sealed pool should have been limited to MAX_POOLED segments
		// The remaining segments should have been deallocated

		// Phase 3: Refill to verify pool works and deallocation happened
		println!("\n=== Phase 3: Refill to Test Pool ===");
		let refill_items = 64 * 15; // 15 segments worth
		for i in 0..refill_items {
			producer.try_push(i + 1000).unwrap();
		}

		let fresh = producer.fresh_allocations();
		let reused = producer.pool_reuses();

		println!("After refilling {} items:", refill_items);
		println!("  Total fresh allocations: {}", fresh);
		println!("  Total pool reuses: {}", reused);
		println!("  Pool reuse rate: {:.1}%", producer.pool_reuse_rate());

		// We should have some reuses (from the limited pool)
		// and some fresh allocations (because pool was limited)
		assert!(
			reused > 0,
			"Should have some pool reuses from the {} pooled segments",
			MAX_POOLED
		);
		// Note: pool_reuses can be MAX_POOLED + 1 due to the optimization where
		// we reuse the last segment during deallocation instead of freeing and
		// immediately reallocating it. This is acceptable.
		assert!(
			reused <= MAX_POOLED as u64 + 1,
			"Should not reuse significantly more than max pooled segments (got {}, max {})",
			reused,
			MAX_POOLED
		);

		println!("\nBounded pool deallocation working correctly!");
	}

	#[test]
	fn test_producer_at_consumer_heel() {
		// Test the case where producer fills queue almost to capacity
		// (producer at consumer's heel) and consumer starts draining
		const TEST_P: usize = 6; // 64 items/segment
		const TEST_NUM_SEGS_P2: usize = 4; // 16 segments (1024 capacity)
		type TestQ = Spsc<u64, TEST_P, TEST_NUM_SEGS_P2>;

		const MAX_POOLED: usize = 4;
		let (producer, consumer) = TestQ::new_with_config(MAX_POOLED);

		let capacity = TestQ::capacity();

		// Fill to near capacity (producer at heel of consumer)
		println!("\n=== Filling to Near Capacity ===");
		for i in 0..(capacity - 10) {
			producer.try_push(i as u64).unwrap();
		}
		println!("Filled to {} items (capacity: {})", capacity - 10, capacity);
		println!("Queue length: {}", producer.len());

		// Now drain everything - should safely deallocate excess segments
		println!("\n=== Draining While Managing Pool ===");
		let mut drained = 0;
		while let Some(_) = consumer.try_pop() {
			drained += 1;
		}
		println!("Drained {} items", drained);

		// Verify queue is empty
		assert_eq!(consumer.len(), 0, "Queue should be empty");
		assert!(
			consumer.try_pop().is_none(),
			"Should not be able to pop from empty queue"
		);

		// Refill to verify everything still works
		println!("\n=== Refill After Deallocation ===");
		for i in 0..200 {
			producer.try_push(i).unwrap();
		}

		let count = consumer.len();
		assert_eq!(count, 200, "Should have 200 items in queue");

		println!("Successfully handled producer at consumer heel!");
	}

	#[test]
	fn test_deallocation_disabled() {
		// Test that max_pooled_segments=usize::MAX disables deallocation
		const TEST_P: usize = 6;
		const TEST_NUM_SEGS_P2: usize = 6;
		type TestQ = Spsc<u64, TEST_P, TEST_NUM_SEGS_P2>;

		let (producer, consumer) = TestQ::new_with_config(usize::MAX); // usize::MAX = disabled

		// Fill and drain many times
		for cycle in 0..5 {
			for i in 0..500 {
				producer.try_push(i).unwrap();
			}
			while consumer.try_pop().is_some() {}
		}

		// With deallocation disabled, we should see high reuse rates
		let reuse_rate = producer.pool_reuse_rate();
		println!("Reuse rate with deallocation disabled: {:.1}%", reuse_rate);

		// After warm-up, should have excellent reuse
		assert!(
			reuse_rate > 50.0,
			"Should have good reuse rate when deallocation disabled"
		);

		println!("Deallocation disabled mode works correctly!");
	}

	#[test]
	fn test_deallocate_to_zero() {
		// Test that max_pooled_segments=0 deallocates all segments
		const TEST_P: usize = 6; // 64 items/segment
		const TEST_NUM_SEGS_P2: usize = 6; // 64 segments
		type TestQ = Spsc<u64, TEST_P, TEST_NUM_SEGS_P2>;

		let (producer, consumer) = TestQ::new_with_config(0); // 0 = keep 0 pooled

		println!("\n=== Testing max_pooled_segments = 0 (deallocate all) ===");

		// Fill many segments
		for i in 0..500 {
			producer.try_push(i).unwrap();
		}
		let allocated_after_fill = producer.allocated_segments();
		println!(
			"After filling 500 items: {} segments allocated",
			allocated_after_fill
		);
		assert!(
			allocated_after_fill >= 8,
			"Should have allocated multiple segments"
		);

		// Drain everything
		while consumer.try_pop().is_some() {}

		// With max_pooled_segments=0, automatic deallocation happens on segment boundaries
		// but we can also explicitly trigger it
		let allocated_before_explicit = producer.allocated_segments();
		println!(
			"After drain (before explicit dealloc): {} segments allocated",
			allocated_before_explicit
		);

		// Explicitly deallocate to ensure all segments are freed
		let deallocated = consumer.try_deallocate_excess();
		println!("Explicitly deallocated: {} segments", deallocated);

		let allocated_after_drain = producer.allocated_segments();
		println!(
			"After explicit deallocation: {} segments allocated",
			allocated_after_drain
		);

		// Should deallocate to 0 or 1 (1 may remain if it's the current active segment)
		// The active segment at the current head/tail position may not be in the sealed pool yet
		assert!(
			allocated_after_drain <= 1,
			"Should deallocate almost all segments with max_pooled_segments=0 (got {})",
			allocated_after_drain
		);

		// Verify that fresh allocations will happen on next use (no pooled segments)
		for i in 0..100 {
			producer.try_push(i + 1000).unwrap();
		}
		while consumer.try_pop().is_some() {}

		// Explicitly deallocate again
		consumer.try_deallocate_excess();

		// After another cycle, we should still have minimal segments
		let final_allocated = producer.allocated_segments();
		println!(
			"After another cycle: {} segments allocated",
			final_allocated
		);
		assert!(
			final_allocated <= 2,
			"Should maintain minimal allocation with max_pooled_segments=0 (got {})",
			final_allocated
		);

		println!("Maximum memory reclamation working correctly!");
	}

	#[test]
	fn test_bursty_workload_memory_management() {
		// Simulate realistic bursty workload with varying load
		const TEST_P: usize = 6; // 64 items/segment
		const TEST_NUM_SEGS_P2: usize = 8; // 256 segments
		type TestQ = Spsc<u64, TEST_P, TEST_NUM_SEGS_P2>;

		const MAX_POOLED: usize = 8;
		let (producer, consumer) = TestQ::new_with_config(MAX_POOLED);

		println!("\n=== Simulating Bursty Workload ===");

		// Burst 1: Large spike
		println!("\nBurst 1: Large spike");
		for i in 0..2000 {
			producer.try_push(i).unwrap();
		}
		println!("  Pushed 2000 items");

		// Partial drain
		for _ in 0..1500 {
			consumer.try_pop();
		}
		println!("  Drained 1500 items, {} remaining", producer.len());

		// Burst 2: Medium spike
		println!("\nBurst 2: Medium spike");
		for i in 0..1000 {
			producer.try_push(i).unwrap();
		}
		println!("  Pushed 1000 items, {} total", producer.len());

		// Full drain
		let mut count = 0;
		while consumer.try_pop().is_some() {
			count += 1;
		}
		println!("  Drained {} items", count);

		// Burst 3: Another large spike
		println!("\nBurst 3: Another large spike");
		for i in 0..3000 {
			producer.try_push(i).unwrap();
		}
		println!("  Pushed 3000 items");

		// Drain everything
		count = 0;
		while consumer.try_pop().is_some() {
			count += 1;
		}
		println!("  Drained {} items", count);

		// Check final stats
		println!("\n=== Final Statistics ===");
		println!("Fresh allocations: {}", producer.fresh_allocations());
		println!("Pool reuses: {}", producer.pool_reuses());
		println!("Reuse rate: {:.1}%", producer.pool_reuse_rate());

		assert_eq!(consumer.len(), 0, "Queue should be empty");
		println!("\nBursty workload handled successfully!");
	}

	#[test]
	fn test_allocated_segments_tracking() {
		// Test that allocated_segments() accurately tracks live memory
		const TEST_P: usize = 6; // 64 items/segment
		const TEST_NUM_SEGS_P2: usize = 8; // 256 segments max
		type TestQ = Spsc<u64, TEST_P, TEST_NUM_SEGS_P2>;

		const MAX_POOLED: usize = 8;
		let (producer, consumer) = TestQ::new_with_config(MAX_POOLED);

		println!("\n=== Testing Allocated Segments Tracking ===");

		// Initially no segments allocated
		assert_eq!(
			producer.allocated_segments(),
			0,
			"Should start with 0 allocated segments"
		);
		println!("Initial: {} segments", producer.allocated_segments());

		// Phase 1: Allocate segments by filling
		println!("\n--- Phase 1: Allocating Segments ---");
		let items_per_seg = 64;
		for i in 0..(items_per_seg * 5) {
			producer.try_push(i).unwrap();
		}

		let allocated_after_fill = producer.allocated_segments();
		println!(
			"After filling 5 segments: {} allocated",
			allocated_after_fill
		);
		assert_eq!(allocated_after_fill, 5, "Should have 5 segments allocated");

		// Phase 2: Drain to create sealed pool
		println!("\n--- Phase 2: Draining (Creates Sealed Pool) ---");
		let mut drained = 0;
		while let Some(_) = consumer.try_pop() {
			drained += 1;
		}
		println!("Drained {} items", drained);

		// Segments should still be allocated (in sealed pool)
		let allocated_after_drain = producer.allocated_segments();
		println!("After drain: {} allocated (in pool)", allocated_after_drain);
		// Should be limited by MAX_POOLED
		assert!(
			allocated_after_drain <= MAX_POOLED as u64,
			"Should have at most {} segments after drain, got {}",
			MAX_POOLED,
			allocated_after_drain
		);

		// Phase 3: Fill many more segments (burst)
		println!("\n--- Phase 3: Burst Allocation ---");
		for i in 0..(items_per_seg * 20) {
			producer.try_push(i).unwrap();
		}

		let allocated_peak = producer.allocated_segments();
		println!("Peak allocation: {} segments", allocated_peak);
		assert!(
			allocated_peak >= 20,
			"Should have allocated at least 20 segments during burst"
		);

		// Calculate memory usage
		let memory_bytes = producer.allocated_memory_bytes();
		let memory_kb = memory_bytes / 1024;
		println!("Memory usage: {} bytes ({} KB)", memory_bytes, memory_kb);

		// Verify memory calculation
		let expected_bytes = allocated_peak as usize * 64 * core::mem::size_of::<u64>();
		assert_eq!(
			memory_bytes, expected_bytes,
			"Memory calculation should be accurate"
		);

		// Phase 4: Full drain to create large sealed pool
		println!("\n--- Phase 4: Full Drain (Creates Large Sealed Pool) ---");
		drained = 0;
		while let Some(_) = consumer.try_pop() {
			drained += 1;
		}
		println!("Drained {} items", drained);

		let allocated_before_trigger = producer.allocated_segments();
		println!(
			"Before triggering deallocation: {} segments",
			allocated_before_trigger
		);

		// Phase 5: Trigger deallocation by allocating new segments
		// With producer-side deallocation, segments are deallocated when the producer
		// steals from an oversized pool during allocation
		println!("\n--- Phase 5: Trigger Deallocation ---");
		println!("Pushing items to trigger producer-side deallocation...");
		for i in 0..(items_per_seg * 5) {
			producer.try_push(i).unwrap();
		}

		let allocated_after_dealloc = producer.allocated_segments();
		println!(
			"After triggering deallocation: {} segments",
			allocated_after_dealloc
		);

		// Should be limited by MAX_POOLED now
		assert!(
			allocated_after_dealloc <= MAX_POOLED as u64 + 5, // +5 for newly allocated segments
			"After deallocation trigger, should have at most {} segments, got {}",
			MAX_POOLED + 5,
			allocated_after_dealloc
		);

		println!("\n=== Summary ===");
		println!("Peak segments: {}", allocated_peak);
		println!("Final segments: {}", allocated_after_dealloc);
		println!(
			"Deallocation reduced memory by {} segments",
			allocated_peak.saturating_sub(allocated_after_dealloc)
		);
		println!(
			"Memory reduction: {} KB",
			allocated_peak.saturating_sub(allocated_after_dealloc) as usize * 64 * 8 / 1024
		);

		assert!(
			allocated_peak > allocated_after_dealloc,
			"Deallocation should have reduced segment count from {} to {}",
			allocated_peak,
			allocated_after_dealloc
		);
	}

	#[test]
	fn test_allocated_segments_no_deallocation() {
		// Test that allocated_segments() works correctly when deallocation is disabled
		const TEST_P: usize = 6;
		const TEST_NUM_SEGS_P2: usize = 6;
		type TestQ = Spsc<u64, TEST_P, TEST_NUM_SEGS_P2>;

		let (producer, consumer) = TestQ::new_with_config(0); // Deallocation disabled

		// Allocate some segments
		for i in 0..500 {
			producer.try_push(i).unwrap();
		}

		let allocated_after_fill = producer.allocated_segments();
		println!("After filling: {} segments allocated", allocated_after_fill);

		// Drain everything
		while consumer.try_pop().is_some() {}

		let allocated_after_drain = producer.allocated_segments();
		println!(
			"After drain (no dealloc): {} segments allocated",
			allocated_after_drain
		);

		// With deallocation disabled, segments should remain allocated
		assert_eq!(
			allocated_after_fill, allocated_after_drain,
			"With deallocation disabled, segment count should not decrease"
		);
	}

	#[test]
	fn test_memory_bytes_calculation() {
		// Test the allocated_memory_bytes() calculation
		const TEST_P: usize = 10; // 1024 items/segment
		const TEST_NUM_SEGS_P2: usize = 4;
		type TestQ = Spsc<u32, TEST_P, TEST_NUM_SEGS_P2>;

		let (producer, consumer) = TestQ::new_with_config(4);

		assert_eq!(
			producer.allocated_memory_bytes(),
			0,
			"Should start with 0 bytes"
		);

		// Allocate 3 segments (3 * 1024 items)
		for i in 0..(1024 * 3) {
			producer.try_push(i).unwrap();
		}

		let expected_bytes = 3 * 1024 * core::mem::size_of::<u32>();
		let actual_bytes = producer.allocated_memory_bytes();

		println!("Allocated: 3 segments = {} bytes", actual_bytes);
		assert_eq!(
			actual_bytes, expected_bytes,
			"Memory calculation should be: 3 segments × 1024 items × 4 bytes"
		);
	}

	#[test]
	fn test_consumer_side_deallocation() {
		// Test that consumer can reclaim memory even when producer is idle
		const TEST_P: usize = 6; // 64 items/segment
		const TEST_NUM_SEGS_P2: usize = 8; // 256 segments
		const MAX_POOLED: usize = 8;
		type TestQ = Spsc<u64, TEST_P, TEST_NUM_SEGS_P2>;

		let (producer, consumer) = TestQ::new_with_config(MAX_POOLED);

		println!("\n=== Testing Consumer-Side Deallocation ===");
		println!("Max pooled segments: {}", MAX_POOLED);

		// Phase 1: Producer creates many segments during burst
		println!("\n--- Phase 1: Producer Burst ---");
		let items_per_seg = 64;
		let num_segments = 30;
		for i in 0..(items_per_seg * num_segments) {
			producer.try_push(i).unwrap();
		}

		let allocated_after_burst = producer.allocated_segments();
		println!("After burst: {} segments allocated", allocated_after_burst);
		assert!(
			allocated_after_burst >= num_segments,
			"Should have allocated at least {} segments",
			num_segments
		);

		// Phase 2: Consumer drains everything (producer goes idle)
		println!("\n--- Phase 2: Consumer Drains (Producer Idle) ---");
		let mut drained = 0;
		while let Some(_) = consumer.try_pop() {
			drained += 1;
		}
		println!("Drained {} items", drained);

		// Consumer-side deallocation happens in seal_after() when crossing segment boundaries
		// while the queue is empty. Push and consume items across a segment to trigger it.
		println!("Triggering segment boundary to activate consumer deallocation...");
		for _ in 0..items_per_seg {
			producer.try_push(999).unwrap();
		}
		for _ in 0..items_per_seg {
			consumer.try_pop();
		}

		// Consumer should have deallocated excess segments during consumption
		// without any producer activity
		let allocated_after_consumer_dealloc = producer.allocated_segments();
		println!(
			"After consumer deallocation: {} segments",
			allocated_after_consumer_dealloc
		);

		// The consumer should have brought it down close to max_pooled_segments
		// (+1 for the segment we just used to trigger the boundary)
		assert!(
			allocated_after_consumer_dealloc <= MAX_POOLED as u64 + 1,
			"Consumer should have deallocated down to ~{} segments, but {} remain",
			MAX_POOLED,
			allocated_after_consumer_dealloc
		);

		println!("\n=== Summary ===");
		println!("Peak segments: {}", allocated_after_burst);
		println!("After consumer GC: {}", allocated_after_consumer_dealloc);
		println!(
			"Consumer reclaimed {} segments ({} KB)",
			allocated_after_burst - allocated_after_consumer_dealloc,
			(allocated_after_burst - allocated_after_consumer_dealloc) as usize * 64 * 8 / 1024
		);
		println!("Consumer-side garbage collection working correctly!");
	}

	#[test]
	fn test_consumer_and_producer_deallocation_together() {
		// Test both consumer-side and producer-side deallocation working together
		const TEST_P: usize = 6; // 64 items/segment
		const TEST_NUM_SEGS_P2: usize = 8;
		const MAX_POOLED: usize = 10;
		type TestQ = Spsc<u64, TEST_P, TEST_NUM_SEGS_P2>;

		let (producer, consumer) = TestQ::new_with_config(MAX_POOLED);

		println!("\n=== Testing Combined Deallocation ===");

		let items_per_seg = 64;

		// Scenario: Alternating burst and drain cycles
		for cycle in 0..3 {
			println!("\n--- Cycle {} ---", cycle + 1);

			// Burst: Producer allocates many segments
			let burst_size = 25 + (cycle * 5); // Increasing burst sizes
			for i in 0..(items_per_seg * burst_size) {
				producer.try_push(i).unwrap();
			}

			let after_burst = producer.allocated_segments();
			println!("  After burst: {} segments", after_burst);

			// Partial drain (consumer-side deallocation kicks in)
			let drain_count = items_per_seg * (burst_size - 5);
			for _ in 0..drain_count {
				consumer.try_pop();
			}

			let after_partial_drain = producer.allocated_segments();
			println!("  After partial drain: {} segments", after_partial_drain);

			// Small refill (producer-side deallocation if pool is oversized)
			for i in 0..(items_per_seg * 3) {
				producer.try_push(i).unwrap();
			}

			let after_refill = producer.allocated_segments();
			println!("  After refill: {} segments", after_refill);
		}

		// Final drain
		while let Some(_) = consumer.try_pop() {}

		// Trigger consumer-side deallocation by crossing a segment boundary while empty
		for _ in 0..items_per_seg {
			producer.try_push(999).unwrap();
		}
		for _ in 0..items_per_seg {
			consumer.try_pop();
		}

		let final_allocated = producer.allocated_segments();
		println!("\n=== Final State ===");
		println!("Allocated segments: {}", final_allocated);

		assert!(
			final_allocated <= MAX_POOLED as u64 + 1,
			"Final allocation should be within max_pooled_segments limit (got {})",
			final_allocated
		);

		println!("Combined deallocation working correctly!");
	}
}
